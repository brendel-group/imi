{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42086c42-5465-467d-a353-0cfca15d7d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "from stimuli_generation import utils as sg_utils\n",
    "%cd tools/data_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc27621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"font.family\"] = \"sans-serif\"\n",
    "rcParams[\"font.sans-serif\"] = [\"DejaVu Sans\"]\n",
    "\n",
    "# output text as text and not paths\n",
    "rcParams[\"svg.fonttype\"] = \"none\"\n",
    "rcParams[\"pdf.fonttype\"] = \"truetype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919fbfc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from lucent.optvis.hooks import ModelHook\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72fbb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMAGENET_VAL_PATH = \"/scratch_local/datasets/ImageNet2012/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436e149-3668-475b-9b5c-aec9df9efa3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TORCH_HOME\"] = \"~/.torch\"\n",
    "assert os.path.exists(IMAGENET_VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35195609",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_sparsity(df_model_name: str, main_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute the sparsity of a model's activations.\n",
    "\n",
    "    Args:\n",
    "        model_name: The name of the model to compute the sparsity of.\n",
    "        main_df: The main dataframe to use.\n",
    "\n",
    "    Returns:\n",
    "        A copy of the relevant subset of the supplied dataframe with the sparsity\n",
    "        values added.\n",
    "    \"\"\"\n",
    "    model_name = df_model_name.replace(\"_hard85\", \"\").replace(\"_hard95\", \"\")\n",
    "\n",
    "    val_loader = sg_utils.get_dataloader(IMAGENET_VAL_PATH, model_name, batch_size=128)\n",
    "    model = sg_utils.load_model(model_name)\n",
    "\n",
    "    if hasattr(model, \"visual\"):\n",
    "        model = model.visual\n",
    "\n",
    "    model_df = main_df[main_df[\"model\"] == model_name].copy().reset_index(drop=True)\n",
    "    model_layers = list(model_df.layer.unique())\n",
    "\n",
    "    pixel_sparsity_values, channel_sparsity_values = count_sparsity(\n",
    "        model, model_layers, val_loader)\n",
    "\n",
    "    for k, v in zip(\n",
    "            (\"pixel_sparsity\", \"channel_sparsity\"),\n",
    "            (pixel_sparsity_values, channel_sparsity_values)\n",
    "    ):\n",
    "        model_df[f\"mean_validation_{k}\"] = model_df.apply(\n",
    "            lambda row: v[row[\"layer\"]][row[\"channel\"]], axis=1)\n",
    "        model_df[f\"validation_{k}\"] = None\n",
    "\n",
    "    return model_df\n",
    "\n",
    "def count_sparsity(model, layers, val_loader):\n",
    "    \"\"\"Count the sparsity of the activations of a model's layers.\n",
    "\n",
    "    Args:\n",
    "        model: The model to count the sparsity of.\n",
    "        layers: The layers to count the sparsity of.\n",
    "        val_loader: The validation loader to use.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of dictionaries, one for pixelwise sparsity and one for channel\n",
    "        sparsity. Each dictionary maps layer names to a numpy array of sparsity\n",
    "        values. Note that these arrays represent per-image values, i.e., they still\n",
    "        contain a batch dimension.\n",
    "    \"\"\"\n",
    "    pixel_sparsity_values = {}\n",
    "    channel_sparsity_values = {}\n",
    "\n",
    "    for k in layers:\n",
    "        pixel_sparsity_values[k] = []\n",
    "        channel_sparsity_values[k] = []\n",
    "\n",
    "    hook_layer_names = [l if not l.startswith(\"visual_\") else l[len(\"visual_\"):] for l in layers]\n",
    "\n",
    "    with ModelHook(model, layer_names=hook_layer_names) as hook:\n",
    "        with torch.no_grad():\n",
    "            for x, _, _ in tqdm(val_loader, leave=False):\n",
    "                x = x.to(device)\n",
    "                model(x)\n",
    "                for ln, hln in zip(layers, hook_layer_names):\n",
    "                    act = hook(hln)\n",
    "                    if act.shape[1] < act.shape[-1]:\n",
    "                        if act.ndim == 4:\n",
    "                            act = torch.permute(act, (0, 3, 1, 2)).contiguous()\n",
    "                        else:\n",
    "                            act = torch.permute(act, (0, 2, 1)).contiguous()\n",
    "                    if act.ndim == 4:\n",
    "                        act = act.view(*act.shape[:-2], -1)\n",
    "                    pixel_sparsity = (act <= 0).float().mean(-1).cpu().numpy()\n",
    "                    channel_sparsity = torch.all(act <= 0, -1).float().cpu().numpy()\n",
    "                    pixel_sparsity_values[ln].append(pixel_sparsity)\n",
    "                    channel_sparsity_values[ln].append(channel_sparsity)\n",
    "\n",
    "    for sparsity_values in (pixel_sparsity_values, channel_sparsity_values):\n",
    "        for k in layers:\n",
    "            sparsity_values[k] = np.mean(np.concatenate(sparsity_values[k], 0), 0).copy()\n",
    "\n",
    "    return pixel_sparsity_values, channel_sparsity_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c6d711-e833-4bc8-a36d-345d29e8f9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dummy loading all models.\n",
    "# We do this to see if every model can properly be loaded on this device.\n",
    "for model_name in sg_utils.accuracies.keys():\n",
    "    sg_utils.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328b96b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "main_df = pd.read_pickle(\"responses_main.pd.pkl\")\n",
    "relevant_columns = [\"model\", \"layer\", \"channel\", \"mode\"]\n",
    "main_df = main_df[(~main_df[\"is_demo\"]) & (~main_df[\"catch_trial\"])]\n",
    "main_df = main_df.drop(\n",
    "    [c for c in main_df.columns if c not in relevant_columns + [\"correct\"]], axis=1\n",
    ").groupby(\n",
    "    relevant_columns, as_index=False).mean(numeric_only=True).reset_index(drop=True)\n",
    "main_df[\"channel\"] = main_df[\"channel\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226f2f5-a518-4ccb-a075-80023f3652c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_main_df = pd.concat(\n",
    "    [compute_sparsity(model_name, main_df) for model_name in tqdm(main_df[\"model\"].unique())],\n",
    "    axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "updated_main_df.to_pickle(\"responses_main_with_sparsity.pd.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2774df9-ea01-45a9-8d12-60cdeaac1eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_main_df = pd.read_pickle(\"responses_main_with_sparsity.pd.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61fea3-3fd7-46fd-9e5c-461b9cdbb382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "model_name_lut = collections.OrderedDict()\n",
    "model_name_lut[\"googlenet\"] = \"GoogLeNet\"\n",
    "model_name_lut[\"densenet_201\"] = \"DenseNet\"\n",
    "model_name_lut[\"resnet50\"] = \"ResNet\"\n",
    "model_name_lut[\"resnet50-l2\"] = \"Robust ResNet\"\n",
    "model_name_lut[\"clip-resnet50\"] = \"Clip ResNet\"\n",
    "model_name_lut[\"wide_resnet50\"] = \"WideResNet\"\n",
    "model_name_lut[\"in1k-vit_b32\"] = \"ViT\"\n",
    "model_name_lut[\"clip-vit_b32\"] = \"Clip ViT\"\n",
    "model_name_lut[\"convnext_b\"] = \"ConvNeXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06175fd7-1fa8-40c5-a8f6-407148a419e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc3f9c-8b78-4304-ad4a-dfe095f76245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, s, t in zip((\"mean_validation_channel_sparsity\", \"mean_validation_pixel_sparsity\"),\n",
    "                   (\"channel_sparsity\", \"pixel_sparsity\"),\n",
    "                   (\"Channelwise Sparsity\", \"Pixelwise Sparsity\")):\n",
    "    n_cols = 3\n",
    "    n_rows = int(np.ceil(len(model_name_lut) / n_cols))\n",
    "    fig, ax = plt.subplots(n_rows, n_cols, sharey=True)\n",
    "\n",
    "    ax = ax.flatten()\n",
    "    fig.set_size_inches(n_cols*4*0.9, n_rows*3*0.9)\n",
    "    for i, m in enumerate(model_name_lut.keys()):\n",
    "        relevant_df = updated_main_df[updated_main_df[\"model\"] == m]\n",
    "        relevant_df = relevant_df[relevant_df[\"mode\"] == \"natural\"]\n",
    "        stats = scipy.stats.spearmanr(relevant_df[k], relevant_df[\"correct\"])\n",
    "        ax[i].set_title(\n",
    "            f\"\"\"{model_name_lut[m]} ($\\\\rho = {stats.statistic:.2f}, p = {stats.pvalue:.2f}$)\"\"\")\n",
    "        ax[i].scatter(relevant_df[k], relevant_df[\"correct\"], s=5, clip_on=False, color=f\"C{i}\")\n",
    "        ax[i].plotted = True\n",
    "        ax[i].spines[\"top\"].set_visible(False)\n",
    "        ax[i].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        ax[i].spines[\"left\"].set_bounds(0.3, 1)\n",
    "        ax[i].set_ylim(0.275, 1)\n",
    "\n",
    "        ax[i].spines[\"bottom\"].set_bounds(0.0, 1)\n",
    "        ax[i].set_xlim(-0.025, 1)\n",
    "\n",
    "        ax[i].tick_params(axis='both', which='both', labelsize=11)\n",
    "\n",
    "    ax[3].set_ylabel(\"Proportion Correct (Natural)\", fontsize=12)\n",
    "\n",
    "    ax[-2].set_xlabel(t, fontsize=12)\n",
    "    for a in ax:\n",
    "        if not hasattr(a, \"plotted\"):\n",
    "            a.axis(\"off\")\n",
    "    plt.tight_layout(rect=[0, 0.0, 1, 0.99])\n",
    "    plt.savefig(f\"results/imi_{s}_natural.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ee441-a076-4600-8c01-b9638664a81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
