{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a42031",
   "metadata": {},
   "source": [
    "## Analysis of relationship between local contrast and IMI score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42086c42-5465-467d-a353-0cfca15d7d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "from stimuli_generation import utils as sg_utils\n",
    "%cd tools/data_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919fbfc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from lucent.optvis.hooks import ModelHook\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"font.family\"] = \"sans-serif\"\n",
    "rcParams[\"font.sans-serif\"] = [\"DejaVu Sans\"]\n",
    "\n",
    "# output text as text and not paths\n",
    "rcParams[\"svg.fonttype\"] = \"none\"\n",
    "rcParams[\"pdf.fonttype\"] = \"truetype\"\n",
    "\n",
    "colors = {\n",
    "    \"synthetic\": [71 / 255, 120 / 255, 158 / 255], \n",
    "    \"natural\": [255 / 255, 172 / 255, 116 / 255],\n",
    "\n",
    "    \"natural easy\": [255 / 255, 172 / 255, 116 / 255],\n",
    "    \"natural medium\": [197 / 255, 135 / 255, 96 / 255],\n",
    "    \"natural hard\": [150 / 255, 105 / 255, 75 / 255],\n",
    "    \"natural very hard\": [109 / 255, 75 / 255, 52 / 255],\n",
    "\n",
    "    \"synthetic easy\": [71 / 255, 120 / 255, 158 / 255], \n",
    "    \"synthetic medium\": [52 / 255, 81 / 255, 105 / 255],\n",
    "    \"synthetic very hard\": [39 / 255, 61 / 255, 79 / 255], \n",
    "\n",
    "    \"natural low\": [255 / 255, 172 / 255, 116 / 255],\n",
    "    \"natural high\": [146 / 255, 100 / 255, 71 / 255],\n",
    "\n",
    "    \"synthetic low\": [71 / 255, 120 / 255, 158 / 255], \n",
    "    \"synthetic high\": [39 / 255, 61 / 255, 79 / 255], \n",
    "\n",
    "    \"c0\": [245 / 255, 181 / 255, 121 / 255],\n",
    "    \"c1\": [244 / 255, 170 / 255, 113 / 255],\n",
    "    \"c2\": [203 / 255, 147 / 255, 94 / 255],\n",
    "    \"c3\": [196 / 255, 134 / 255, 91 / 255],\n",
    "    \"c4\": [150 / 255, 108 / 255, 68 / 255],\n",
    "    \"c5\": [142 / 255, 98 / 255, 67 / 255],\n",
    "    \"c6\": [116 / 255, 72 / 255, 44 / 255],\n",
    "    \"c7\": [103 / 255, 76 / 255, 54 / 255],\n",
    "    \"c8\":  [88 / 255, 73 / 255, 59 / 255],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72fbb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMAGENET_VAL_PATH = os.path.join(sg_utils.IMAGENET_PATH, \"val\")\n",
    "assert os.path.exists(IMAGENET_VAL_PATH)\n",
    "output_dir = \"/mnt/qb/work/bethge/tklein16/int_comp_out/contrast/\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_batch_contrast_torch(acts):\n",
    "    \"\"\"\n",
    "    Computes the average local contrast for a batch of activations on the GPU.\n",
    "    See last cell for an explanation by example.\n",
    "\n",
    "    :param acts: tensor containing activations for a batch and layer, shape (bs x n_units x h x w)\n",
    "    :returns: tensor of average local contrasts of shape (bs x n_units)\n",
    "    \"\"\"\n",
    "\n",
    "    assert acts.dim() == 4, \"Expected a 4d tensor!\"\n",
    "\n",
    "    # pad each image with a border of 0s\n",
    "    p2d = (1, 1, 1, 1)  # pad last dim by (1, 1) and 2nd to last by (1,1)\n",
    "    acts = F.pad(acts, p2d, \"constant\", 0)\n",
    "\n",
    "    # make tensor to hold results\n",
    "    res = torch.zeros_like(acts)\n",
    "\n",
    "    # roll image left, right, up, down\n",
    "    for ax in [-1, -2]:\n",
    "        for shift in [1, -1]:\n",
    "            rolled = torch.roll(acts, shifts=shift, dims=ax)\n",
    "            res += torch.abs(rolled - acts)\n",
    "\n",
    "    # cut off the padding\n",
    "    res = res[:, :, 1:-1, 1:-1]\n",
    "\n",
    "    # calculate the averages\n",
    "    res = torch.mean(res, dim=(-2, -1))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def compute_local_contrast(df_model_name: str, main_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the local contrast of a model's activations.\n",
    "\n",
    "    Args:\n",
    "        model_name: The name of the model to compute the sparsity of.\n",
    "        main_df: The main dataframe to use.\n",
    "\n",
    "    Returns:\n",
    "        A copy of the relevant subset of the supplied dataframe with the contrast\n",
    "        values added.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating local contrast for model {df_model_name}\")\n",
    "    model_name = df_model_name.replace(\"_hard85\", \"\").replace(\"_hard95\", \"\")\n",
    "\n",
    "    val_loader = sg_utils.get_dataloader(IMAGENET_VAL_PATH, model_name, batch_size=64)\n",
    "    model = sg_utils.load_model(model_name)\n",
    "\n",
    "    if hasattr(model, \"visual\"):\n",
    "        model = model.visual\n",
    "\n",
    "    model_df = main_df[main_df[\"model\"] == model_name].copy().reset_index(drop=True)\n",
    "    model_layers = list(model_df.layer.unique())\n",
    "\n",
    "    channel_local_contrast = calc_contrast(\n",
    "        model_name, model, model_layers, val_loader\n",
    "    )\n",
    "\n",
    "    model_df[\"mean_validation_contrast\"] = model_df.apply(\n",
    "        lambda row: channel_local_contrast[row[\"layer\"]][row[\"channel\"]], axis=1)\n",
    "\n",
    "    return model_df\n",
    "\n",
    "\n",
    "def calc_contrast(model_name, model, layers, val_loader):\n",
    "    \"\"\"\n",
    "    Calculate the local contrast of the activations of a model's layers.\n",
    "\n",
    "    Args:\n",
    "        model_name: The name of the model\n",
    "        model: The model for which contrast is calculated.\n",
    "        layers: The layers for which contrast is calculated.\n",
    "        val_loader: The validation loader to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple containing\n",
    "        - A dictionary that maps layer names to a numpy array of contrast\n",
    "          values. Note that these arrays represent per-image values, i.e., they still\n",
    "          contain a batch dimension.\n",
    "        - A list of layer names containing only the conv-layers\n",
    "    \"\"\"\n",
    "    channel_contrast_values = {\n",
    "        layer: [] for layer in layers\n",
    "    }\n",
    "\n",
    "    hook_layer_names = [l if not l.startswith(\"visual_\") else l[len(\"visual_\"):] for l in layers]\n",
    "\n",
    "    with ModelHook(model, layer_names=hook_layer_names) as hook:\n",
    "        with torch.no_grad():\n",
    "            for x, _, _ in tqdm(val_loader):\n",
    "                x = x.to(sg_utils.DEVICE)\n",
    "                model(x)\n",
    "                for ln, hln in zip(layers, hook_layer_names):\n",
    "                    act = hook(hln)\n",
    "\n",
    "                    # some layers need to be permuted from (b,h,w,c) to the canonical (b,c,h,w)\n",
    "                    if sg_utils.test_permute(model_name, ln):\n",
    "                        act = torch.permute(act, (0, 3, 1, 2)).contiguous()\n",
    "\n",
    "                    channel_contrast = compute_batch_contrast_torch(act)\n",
    "                    channel_contrast_values[ln].append(channel_contrast.numpy(force=True))\n",
    "\n",
    "    for layer in layers:\n",
    "        channel_contrast_values[layer] = np.mean(np.concatenate(channel_contrast_values[layer], 0), 0).copy()\n",
    "\n",
    "    return channel_contrast_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328b96b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "main_df = pd.read_csv(\"/mnt/qb/bethge/shared/interpretability_comparison/IMI/responses_main.csv\") #pd.read_pickle(\"responses_main.pd.pkl\")\n",
    "relevant_columns = [\"model\", \"layer\", \"channel\", \"mode\"]\n",
    "main_df = main_df[(~main_df[\"is_demo\"]) & (~main_df[\"catch_trial\"])]\n",
    "main_df = main_df.drop(\n",
    "    [c for c in main_df.columns if c not in relevant_columns + [\"correct\"]], axis=1\n",
    ").groupby(\n",
    "    relevant_columns, as_index=False).mean(numeric_only=True).reset_index(drop=True)\n",
    "main_df[\"channel\"] = main_df[\"channel\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98709e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate all models that are not ViTs, and ignore the 'hard' models, as those are redundant\n",
    "relevant_models = [m for m in main_df[\"model\"].unique() if ('vit' not in m and 'hard' not in m)]\n",
    "print(\"Will analyze models\", relevant_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d06745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate contrast for all models, write each one to its own pickle file\n",
    "for model_name in relevant_models:\n",
    "    model_contrast_df = compute_local_contrast(model_name, main_df)\n",
    "    model_contrast_df.to_pickle(\n",
    "        os.path.join(\n",
    "            output_dir,\n",
    "            f\"local_contrast_{model_name}.pd.pkl\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226f2f5-a518-4ccb-a075-80023f3652c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in the pickle-files, concat them to final df\n",
    "updated_main_df = pd.concat(\n",
    "    [pd.read_pickle(\n",
    "        os.path.join(output_dir, f\"local_contrast_{model_name}.pd.pkl\")\n",
    "    ) for model_name in relevant_models],\n",
    "    axis=0, ignore_index=True\n",
    ").reset_index(drop=True)\n",
    "\n",
    "updated_main_df.to_pickle(\n",
    "    os.path.join(\n",
    "        output_dir,\n",
    "        \"responses_main_with_local_contrast.pd.pkl\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e09cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the main-df from pickle\n",
    "updated_main_df = pd.read_pickle(\n",
    "    os.path.join(\n",
    "        output_dir,\n",
    "        \"responses_main_with_local_contrast.pd.pkl\"\n",
    "    )\n",
    ")\n",
    "print(\"Read main-df with models:\", updated_main_df.model.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc75e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "model_name_lut = collections.OrderedDict()\n",
    "model_name_lut[\"googlenet\"] = \"GoogLeNet\"\n",
    "model_name_lut[\"densenet_201\"] = \"DenseNet\"\n",
    "model_name_lut[\"resnet50\"] = \"ResNet\"\n",
    "model_name_lut[\"resnet50-l2\"] = \"Robust ResNet\"\n",
    "model_name_lut[\"clip-resnet50\"] = \"Clip ResNet\"\n",
    "model_name_lut[\"wide_resnet50\"] = \"WideResNet\"\n",
    "model_name_lut[\"convnext_b\"] = \"ConvNeXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5471aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = \"mean_validation_contrast\"\n",
    "s = \"local_contrast\"\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(model_name_lut) / n_cols))\n",
    "fig, ax = plt.subplots(n_rows, n_cols, sharey=True)\n",
    "ax = ax.flatten()\n",
    "fig.set_size_inches(n_cols*4*0.9, n_rows*3*0.9)\n",
    "keys = list(model_name_lut.keys())\n",
    "keys.insert(-1, 'none') # to skip lower left corner and have last plot centered\n",
    "for i, m in enumerate(keys):\n",
    "    if m == \"none\":\n",
    "        i += 1\n",
    "        continue\n",
    "    relevant_df = updated_main_df[updated_main_df[\"model\"] == m]\n",
    "    relevant_df = relevant_df[relevant_df[\"mode\"] == \"natural\"]\n",
    "    stats = scipy.stats.spearmanr(relevant_df[k], relevant_df[\"correct\"])\n",
    "    ax[i].set_title(\n",
    "        f\"\"\"{model_name_lut[m]} ($\\\\rho = {stats.correlation:.2f}, p = {stats.pvalue:.2f}$)\"\"\")\n",
    "    ax[i].scatter(relevant_df[k], relevant_df[\"correct\"], s=5, clip_on=False, color=f\"C{i}\" if i != 7 else \"C8\")\n",
    "\n",
    "    ax[i].plotted = True\n",
    "    ax[i].spines[\"top\"].set_visible(False)\n",
    "    ax[i].spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax[i].spines[\"left\"].set_bounds(0.3, 1)\n",
    "    ax[i].set_ylim(0.275, 1)\n",
    "\n",
    "    x_max = np.max(relevant_df[k])\n",
    "    ax[i].spines[\"bottom\"].set_bounds(0.0, x_max)\n",
    "    ax[i].set_xlim(-0.025, x_max)\n",
    "\n",
    "    ax[i].tick_params(axis='both', which='both', labelsize=11)\n",
    "\n",
    "ax[3].set_ylabel(\"Proportion Correct (Natural)\", fontsize=12)\n",
    "ax[7].set_xlabel(\"Local Contrast in Activation Maps\", fontsize=12)\n",
    "for a in ax:\n",
    "    if not hasattr(a, \"plotted\"):\n",
    "        a.axis(\"off\")\n",
    "\n",
    "# Adding yticklabels for last plot manually\n",
    "plt.tight_layout(rect=[0, 0.0, 1, 0.99])\n",
    "ax[-2].set_autoscale_on(False)\n",
    "ticks = [0.4, 0.6, 0.8, 1.0]\n",
    "for tick in ticks:\n",
    "    ax[-2].text(\n",
    "        x=-2.2,\n",
    "        y=tick - 0.02, # small offset for some reason\n",
    "        s=str(tick),\n",
    "        fontsize=11\n",
    "    )\n",
    "plt.savefig(f\"results/imi_{s}.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462bf79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
