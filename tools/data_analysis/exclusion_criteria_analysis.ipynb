{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to perform a counterfactual simulation on the exclusion criteria thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "%cd ../..\n",
    "from tools.mturk.mturk import MTurkHIT\n",
    "from tools.mturk.spawn_experiment import get_verify_task_callback\n",
    "%cd tools/data_analysis\n",
    "from utils import utils_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"data/experiment_202303/resnet50_natural_20230325\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stored_results = utils_data.load_results(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results = utils_data.parse_results(stored_results, use_raw_data=False)\n",
    "df_checks = utils_data.parse_check_results(stored_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "structure = utils_data.load_and_parse_trial_structure(os.path.join(\n",
    "    folder, \"structure.json\"))\n",
    "df_results = utils_data.append_trial_structure_to_results(df_results, structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Allows counter-factual simulation on the rejected criteria\n",
    "\n",
    "for i, row in df_checks.iterrows():\n",
    "    if not row[\"passed_checks\"]:\n",
    "        if not row[\"instruction_time_result\"] and row[\"instruction_time_details\"][\"total_time\"] > 15:\n",
    "            df_checks.loc[i, \"instruction_time_result\"] = True\n",
    "            v = True\n",
    "            for k in (\"catch_trials_result\", \"row_variability_result\",\n",
    "                      \"row_variability_result\", \"instruction_time_result\"):\n",
    "                v = v and df_checks.loc[i, k]\n",
    "            df_checks.loc[i, \"passed_checks\"] = v\n",
    "            print(\"Changing overall check status from False to\", df_checks.loc[i, \"passed_checks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = utils_data.append_checks_to_results(df_results, df_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_passed = df_results.copy(deep=True)[df_results[\"passed_checks\"]]\n",
    "df_results_rejected = df_results.copy(deep=True)[~df_results[\"passed_checks\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if len(df_results_passed) == len(df_results_rejected):\n",
    "    print(\"WARNING: Number of rejected trials equals that of passed trials; this could be a bug.\")\n",
    "\n",
    "print(\"only using passed responses:\", df_results_passed[\"correct\"].mean())\n",
    "print(\"only using rejected responses:\", df_results_rejected[\"correct\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "catch_trial_ratio_threshold = 0.8\n",
    "min_total_response_time = 135\n",
    "max_total_response_time = 2500\n",
    "min_instruction_time = 15\n",
    "max_instruction_time = 180\n",
    "row_variability_threshold = 5\n",
    "max_demo_trials_attempts = 3\n",
    "verify_task_callback = get_verify_task_callback(\n",
    "    \"2afc\",\n",
    "    catch_trial_ratio_threshold,\n",
    "    min_total_response_time,\n",
    "    max_total_response_time,\n",
    "    min_instruction_time,\n",
    "    max_instruction_time,\n",
    "    row_variability_threshold,\n",
    "    max_demo_trials_attempts,\n",
    ")\n",
    "\n",
    "# Dummy HIT\n",
    "hit = MTurkHIT(\n",
    "    \"1\",\n",
    "    \"1\",\n",
    "    \"1\",\n",
    "    \"1\",\n",
    "    \"1\",\n",
    "    1,\n",
    "    datetime.datetime.now(),\n",
    "    datetime.datetime.now(),\n",
    "    1,\n",
    "    \"2afc\",\n",
    ")\n",
    "\n",
    "raw_response = stored_results[0].raw_responses[0]\n",
    "verify_task_callback(hit, raw_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results_main = df_results[~df_results[\"catch_trial\"] & ~df_results[\"is_demo\"]]\n",
    "df_results_catch = df_results[~df_results[\"catch_trial\"] & ~df_results[\"is_demo\"]]\n",
    "df_results_demo = df_results[df_results[\"is_demo\"]]\n",
    "\n",
    "df_results_passed_main = df_results_passed[~df_results_passed[\"catch_trial\"] & ~df_results_passed[\"is_demo\"]]\n",
    "df_results_passed_catch = df_results_passed[~df_results_passed[\"catch_trial\"] & ~df_results_passed[\"is_demo\"]]\n",
    "df_results_passed_demo = df_results_passed[df_results_passed[\"is_demo\"]]\n",
    "\n",
    "df_results_rejected_main = df_results_rejected[~df_results_rejected[\"catch_trial\"] & ~df_results_rejected[\"is_demo\"]]\n",
    "df_results_rejected_catch = df_results_rejected[df_results_rejected[\"catch_trial\"] & ~df_results_rejected[\"is_demo\"]]\n",
    "df_results_rejected_demo = df_results_rejected[df_results_rejected[\"is_demo\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_results_passed_main.shape, df_results_rejected_main.shape)\n",
    "print(df_results_passed_main[\"correct\"].mean(), df_results_rejected_main[\"correct\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import utils_analysis\n",
    "utils_analysis.apply_all_checks(df_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df_demo = df_results_passed_demo[[\"participant_id\", \"correct\"]].groupby(\"participant_id\").mean()\n",
    "# df_demo = df_demo.rename(columns={'correct':'correct_demo'})\n",
    "\n",
    "df_demo = df_checks[[\"worker_id\", \"demo_trials_details_extracted\"]].rename(\n",
    "    columns={\"worker_id\": \"participant_id\", \"demo_trials_details_extracted\": \"demo_repetitions\"})\n",
    "df_demo = df_demo.set_index(\"participant_id\")\n",
    "\n",
    "df_main = df_results_passed[[\"participant_id\", \"correct\"]].groupby(\"participant_id\").mean()\n",
    "df_main = df_main.rename(columns={'correct':'correct_main'})\n",
    "df_merged = pd.concat((df_main, df_demo), axis=1)\n",
    "\n",
    "plt.scatter(df_merged[\"demo_repetitions\"], df_merged[\"correct_main\"])\n",
    "plt.xlabel(\"#demo repetitions\")\n",
    "plt.ylabel(\"main performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keys = (\"catch_trials_result\", \"row_variability_result\",\n",
    "        \"row_variability_result\", \"instruction_time_result\",\n",
    "        \"demo_trials_result\")\n",
    "fig, axs = plt.subplots(1, len(keys), figsize=(2.5*len(keys), 3))\n",
    "axs = axs.flatten()\n",
    "for k, ax in zip(keys, axs):\n",
    "    df_checks[k].value_counts().plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(k)\n",
    "    ax.set_xlabel(\"Passed Exclusion Criteria\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keys = ('instruction_time_details_extracted',\n",
    "       'total_response_time_details_extracted',\n",
    "       'row_variability_details_details_upper_extracted',\n",
    "       'row_variability_details_details_lower_extracted',\n",
    "       'catch_trials_details_ratio_extracted',\n",
    "       'demo_trials_details_extracted')\n",
    "fig, axs = plt.subplots(int(np.ceil(len(keys) / 3)), 3, figsize=(8, 5))\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "for k, ax in zip(keys, axs):\n",
    "    ax.axis(\"on\")\n",
    "    ax.hist(df_checks[k], bins=20)\n",
    "    ax.set_title(k.replace(\"_extracted\", \"\"))\n",
    "    ax.set_xlabel(\"Passed Exclusion Criteria\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation b/w mean/min/max RT and Accuracy per Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f(df):\n",
    "    accuracy = df[\"correct\"].mean()\n",
    "    min_rt = df[\"rt\"].min()\n",
    "    max_rt = df[\"rt\"].max()\n",
    "    mean_rt = df[\"rt\"].mean()\n",
    "    median_rt = df[\"rt\"].median()\n",
    "    return pd.Series([accuracy, min_rt, max_rt, mean_rt, median_rt],\n",
    "                     index=[\"accuracy\", \"min_rt\", \"max_rt\", \"mean_rt\", \"median_rt\"])\n",
    "pdf_results = df_results.groupby(\"participant_id\").apply(f)\n",
    "\n",
    "plt.scatter(pdf_results[\"min_rt\"] / 1000, pdf_results[\"accuracy\"], label=\"min\")\n",
    "plt.scatter(pdf_results[\"mean_rt\"] / 1000, pdf_results[\"accuracy\"], label=\"mean\")\n",
    "# plt.scatter(pdf_results[\"median_rt\"] / 1000, pdf_results[\"accuracy\"], label=\"median\")\n",
    "#plt.scatter(pdf_results[\"max_rt\"] / 1000, pdf_results[\"accuracy\"], label=\"max\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Reaction Time [s]\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation b/w RT and Correctness of Individual Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(df_results[\"rt\"] / 1000, df_results[\"correct\"])\n",
    "plt.xlabel(\"Reaction Time [s]\")\n",
    "plt.ylabel(\"Trial Correctly Solved?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
