{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import importlib\n",
    "import matplotlib\n",
    "import scikit_posthocs\n",
    "\n",
    "%cd ../..\n",
    "from tools.mturk.mturk import MTurkHIT\n",
    "from tools.mturk.spawn_experiment import get_verify_task_callback\n",
    "from stimuli_generation import sg_utils as utils_stimuli_generation\n",
    "%cd tools/data_analysis\n",
    "from utils import utils_data\n",
    "from utils import utils_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"font.family\"] = \"sans-serif\"\n",
    "rcParams[\"font.sans-serif\"] = [\"DejaVu Sans\"]\n",
    "\n",
    "# output text as text and not paths\n",
    "rcParams[\"svg.fonttype\"] = \"none\"\n",
    "rcParams[\"pdf.fonttype\"] = \"truetype\"\n",
    "\n",
    "colors = {\n",
    "    \"synthetic\": [71 / 255, 120 / 255, 158 / 255], \n",
    "    \"natural\": [255 / 255, 172 / 255, 116 / 255],\n",
    "\n",
    "    \"natural easy\": [255 / 255, 172 / 255, 116 / 255],\n",
    "    \"natural medium\": [197 / 255, 135 / 255, 96 / 255],\n",
    "    \"natural hard\": [150 / 255, 105 / 255, 75 / 255],\n",
    "    \"natural very hard\": [109 / 255, 75 / 255, 52 / 255],\n",
    "\n",
    "    \"synthetic easy\": [71 / 255, 120 / 255, 158 / 255], \n",
    "    \"synthetic medium\": [52 / 255, 81 / 255, 105 / 255],\n",
    "    \"synthetic very hard\": [39 / 255, 61 / 255, 79 / 255], \n",
    "\n",
    "    \"natural low\": [255 / 255, 172 / 255, 116 / 255],\n",
    "    \"natural high\": [146 / 255, 100 / 255, 71 / 255],\n",
    "\n",
    "    \"synthetic low\": [71 / 255, 120 / 255, 158 / 255], \n",
    "    \"synthetic high\": [39 / 255, 61 / 255, 79 / 255], \n",
    "\n",
    "    \"c0\": [245 / 255, 181 / 255, 121 / 255],\n",
    "    \"c1\": [244 / 255, 170 / 255, 113 / 255],\n",
    "    \"c2\": [203 / 255, 147 / 255, 94 / 255],\n",
    "    \"c3\": [196 / 255, 134 / 255, 91 / 255],\n",
    "    \"c4\": [150 / 255, 108 / 255, 68 / 255],\n",
    "    \"c5\": [142 / 255, 98 / 255, 67 / 255],\n",
    "    \"c6\": [116 / 255, 72 / 255, 44 / 255],\n",
    "    \"c7\": [103 / 255, 76 / 255, 54 / 255],\n",
    "    \"c8\":  [88 / 255, 73 / 255, 59 / 255],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"GoogLeNet (natural)\": \"data/experiment_202303/googlenet_natural_20230312\",\n",
    "    \"GoogLeNet (synthetic)\": \"data/experiment_202303/googlenet_optimized_20230312\",\n",
    "\n",
    "    \"DenseNet (natural)\": \"data/experiment_202303/densenet_201_natural_20230412\",\n",
    "    \"DenseNet (synthetic)\": \"data/experiment_202303/densenet_201_optimized_20230503\",\n",
    "\n",
    "    \"ResNet (natural)\": \"data/experiment_202303/resnet50_natural_20230325\",\n",
    "    \"ResNet (synthetic)\": \"data/experiment_202303/resnet50_optimized_20230325\",\n",
    "\n",
    "    \"Hard85 ResNet (natural)\": \"data/experiment_202303/resnet50_hard85_natural_20230505\",\n",
    "    \"Hard85 ResNet (synthetic)\": None,\n",
    "\n",
    "    \"Hard95 ResNet (natural)\": \"data/experiment_202303/resnet50_hard95_natural_20230510\",\n",
    "    \"Hard95 ResNet (synthetic)\": \"data/experiment_202303/resnet50_hard95_optimized_20230514\",\n",
    "\n",
    "    \"Hard99 ResNet (natural)\": \"data/experiment_202303/resnet50_hard99_natural_20230803\",\n",
    "    \"Hard99 ResNet (synthetic)\": None,\n",
    "\n",
    "    \"Robust ResNet (natural)\": \"data/experiment_202303/resnet50-l2_natural_20230314\",\n",
    "    \"Robust ResNet (synthetic)\": \"data/experiment_202303/resnet50-l2_optimized_20230314\",\n",
    "\n",
    "    \"Clip ResNet (natural)\": \"data/experiment_202303/clip-resnet50_natural_20230412\",\n",
    "    \"Clip ResNet (synthetic)\": \"data/experiment_202303/clip-resnet50_optimized_20230502\",\n",
    "\n",
    "    \"Hard85 Clip ResNet (natural)\": \"data/experiment_202303/clip-resnet50_hard85_natural_20230505\",\n",
    "    \"Hard85 Clip ResNet (synthetic)\": None,\n",
    "\n",
    "    \"Hard95 Clip ResNet (natural)\": \"data/experiment_202303/clip-resnet50_hard95_natural_20230510\",\n",
    "    \"Hard95 Clip ResNet (synthetic)\": None,\n",
    "\n",
    "    \"Hard99 Clip ResNet (natural)\": \"data/experiment_202303/clip-resnet50_hard99_natural_20230803\",\n",
    "    \"Hard99 Clip ResNet (synthetic)\": None,\n",
    "\n",
    "    \"WideResNet (natural)\": \"data/experiment_202303/wide_resnet50_natural_20230412\",\n",
    "    \"WideResNet (synthetic)\": \"data/experiment_202303/wide_resnet50_optimized_20230502\",\n",
    "\n",
    "    \"ViT (natural)\": \"data/experiment_202303/in1k-vit_b32_natural_20230501\",\n",
    "    \"ViT (synthetic)\": None,\n",
    "\n",
    "    \"Clip ViT (natural)\": \"data/experiment_202303/clip-vit_b32_natural_20230501\",\n",
    "    \"Clip ViT (synthetic)\": None,\n",
    "\n",
    "    \"ConvNeXT (natural)\": \"data/experiment_202303/convnext_b_natural_20230429\",\n",
    "    \"ConvNeXT (synthetic)\": \"data/experiment_202303/convnext_b_optimized_20230509\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_results = {}\n",
    "dfs_checks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k in experiments:\n",
    "    if (dfs_checks.get(k, None) is not None and dfs_results.get(k, None) is not None) and (len(dfs_checks.get(k, [])) > 0 and len(dfs_results.get(k, [])) > 0):\n",
    "        continue\n",
    "\n",
    "    if experiments[k] is None:\n",
    "        dfs_results[k] = None\n",
    "        dfs_checks[k] = None\n",
    "        continue\n",
    "\n",
    "    print(k)\n",
    "\n",
    "    results = utils_data.load_results(experiments[k])\n",
    "    structure = utils_data.load_and_parse_trial_structure(os.path.join(experiments[k], \"structure.json\"))\n",
    "    df_results = utils_data.parse_results(results, use_raw_data=False)\n",
    "    df_results = utils_data.append_trial_structure_to_results(df_results, structure)\n",
    "    df_checks = utils_analysis.apply_all_checks(utils_data.parse_check_results(results))\n",
    "    dfs_checks[k] = df_checks\n",
    "\n",
    "    df_results = utils_data.append_checks_to_results(df_results, df_checks)\n",
    "    dfs_results[k] = df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_key = [k for k in experiments if k is not None][0]\n",
    "reference_df_results = dfs_results[reference_key]\n",
    "reference_df_checks = dfs_checks[reference_key]\n",
    "for k in experiments:\n",
    "    if experiments[k] is None:\n",
    "        dfs_results[k] = reference_df_results.copy().head(0)\n",
    "        dfs_checks[k] = reference_df_checks.copy().head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the correct structures have been used and no two experiments use the same model & mode\n",
    "unique_modes_and_models = {k: (set(dfs_results[k][\"mode\"].unique()), set(dfs_results[k][\"model\"].unique())) for k in dfs_results}\n",
    "for k1 in unique_modes_and_models:\n",
    "    for k2 in unique_modes_and_models:\n",
    "        if k1 == k2:\n",
    "            continue\n",
    "\n",
    "        if len(unique_modes_and_models[k1][0]) == 0 or len(unique_modes_and_models[k2][0]) == 0:\n",
    "            continue\n",
    "\n",
    "        if unique_modes_and_models[k1] == unique_modes_and_models[k2]:\n",
    "            print(f\"WARNING: {k1} and {k2} use the same model and mode!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & apply list of flawed responses through multi participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "flawed_tasks_and_participants = pickle.load(open(\"data/experiment_202303_neurips_submission/flawed_tasks_and_participants.pd.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dfs_results:\n",
    "    dfs_results[k][\"multi_participation\"] = False\n",
    "\n",
    "for _, row in flawed_tasks_and_participants.iterrows():\n",
    "    task_id = row[\"task_id\"]\n",
    "    worker_id = row[\"worker_id\"]\n",
    "\n",
    "    # Find relevant responses\n",
    "    for k in dfs_results:\n",
    "        df_results = dfs_results[k]\n",
    "        df_results.loc[(df_results[\"task_id\"] == task_id) & (df_results[\"worker_id\"] == worker_id), \"multi_participation\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfs_results_mp = {k: df[df[\"multi_participation\"]] for k, df in dfs_results.items()}\n",
    "dfs_results_no_mp = {k: df[~df[\"multi_participation\"]] for k, df in dfs_results.items()}\n",
    "\n",
    "dfs_results_main = {k: df[~df[\"catch_trial\"] & ~df[\"is_demo\"]] for k, df in dfs_results_no_mp.items()}\n",
    "dfs_results_catch = {k: df[df[\"catch_trial\"] & ~df[\"is_demo\"]] for k, df in dfs_results_no_mp.items()}\n",
    "dfs_results_demo = {k: df[df[\"is_demo\"]] for k, df in dfs_results_no_mp.items()}\n",
    "\n",
    "dfs_results_no_mp_passed = {k: df[df[\"passed_checks\"]] for k, df in dfs_results_no_mp.items()} \n",
    "\n",
    "dfs_results_main_passed = {k: df[df[\"passed_checks\"]] for k, df in dfs_results_main.items()}\n",
    "dfs_results_catch_passed = {k: df[df[\"passed_checks\"]] for k, df in dfs_results_catch.items()}\n",
    "dfs_results_demo_passed = {k: df[df[\"passed_checks\"]] for k, df in dfs_results_demo.items()}\n",
    "\n",
    "dfs_results_main_rejected = {k: df[~df[\"passed_checks\"]] for k, df in dfs_results_main.items()}\n",
    "dfs_results_catch_rejected = {k: df[~df[\"passed_checks\"]] for k, df in dfs_results_catch.items()}\n",
    "dfs_results_demo_rejected = {k: df[~df[\"passed_checks\"]] for k, df in dfs_results_demo.items()}\n",
    "\n",
    "dfs_results_mp_main = {k: df[~df[\"catch_trial\"] & ~df[\"is_demo\"]] for k, df in dfs_results_mp.items()}\n",
    "dfs_results_mp_catch = {k: df[df[\"catch_trial\"] & ~df[\"is_demo\"]] for k, df in dfs_results_mp.items()}\n",
    "dfs_results_mp_demo = {k: df[df[\"is_demo\"] == True] for k, df in dfs_results_mp.items()}\n",
    "\n",
    "dfs_results_mp_main_passed = {k: df[df[\"passed_checks\"]] for k, df in dfs_results_mp_main.items()}\n",
    "dfs_results_mp_catch_passed = {k: df[df[\"passed_checks\"]] for k, df in dfs_results_mp_catch.items()}\n",
    "dfs_results_mp_demo_passed = {k: df[df[\"passed_checks\"]] for k, df in dfs_results_mp_demo.items()}\n",
    "\n",
    "dfs_results_mp_main_rejected = {k: df[~df[\"passed_checks\"]] for k, df in dfs_results_mp_main.items()}\n",
    "dfs_results_mp_catch_rejected = {k: df[~df[\"passed_checks\"]] for k, df in dfs_results_mp_catch.items()}\n",
    "dfs_results_mp_demo_rejected = {k: df[~df[\"passed_checks\"]] for k, df in dfs_results_mp_demo.items()}\n",
    "\n",
    "dfs_results_mp_or_rejected = {k: df[\n",
    "    ~df[\"passed_checks\"] | df[\"multi_participation\"]] for k, df in dfs_results.items()}\n",
    "dfs_results_main_mp_or_rejected = {k: df[~df[\"catch_trial\"] & ~df[\"is_demo\"] & (\n",
    "    ~df[\"passed_checks\"] | df[\"multi_participation\"])] for k, df in dfs_results.items()}\n",
    "dfs_results_catch_mp_or_rejected = {k: df[df[\"catch_trial\"] & (\n",
    "    ~df[\"passed_checks\"] | df[\"multi_participation\"])] for k, df in dfs_results.items()}\n",
    "dfs_results_demo_mp_or_rejected = {k: df[df[\"is_demo\"] & (\n",
    "    ~df[\"passed_checks\"] | df[\"multi_participation\"])] for k, df in dfs_results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of all responses:\", sum([len(df) for df in dfs_results.values()]))\n",
    "print(\"Number of all clean responses:\", sum([len(df) for df in dfs_results_no_mp.values()]))\n",
    "print(\"Number of all flawed responses:\", sum([len(df) for df in dfs_results_mp.values()]))\n",
    "print()\n",
    "\n",
    "print(\"Number of all clean responses:\", sum([len(df) for df in dfs_results_no_mp.values()]))\n",
    "print(\"Number of all clean main responses:\", sum([len(df) for df in dfs_results_main.values()]))\n",
    "print(\"Number of all clean passed main responses:\", sum([len(df) for df in dfs_results_main_passed.values()]))\n",
    "print()\n",
    "\n",
    "print(\"Number of all flawed responses:\", sum([len(df) for df in dfs_results_mp.values()]))\n",
    "print(\"Number of all flawed main responses:\", sum([len(df) for df in dfs_results_mp_main.values()]))\n",
    "print(\"Number of all flawed passed main responses:\", sum([len(df) for df in dfs_results_mp_main_passed.values()]))\n",
    "print()\n",
    "\n",
    "print(\"Number of all passed responses:\", sum([len(df) for df in dfs_results_no_mp_passed.values()]))\n",
    "print(\"Number of all flawed/rejected responses:\", sum([len(df) for df in dfs_results_mp_or_rejected.values()]))\n",
    "print(\"Number of all flawed/rejected main responses:\", sum([len(df) for df in dfs_results_main_mp_or_rejected.values()]))\n",
    "print()\n",
    "\n",
    "print(\"Number of all participants:\", sum([len(df[\"worker_id\"].unique()) for df in dfs_results.values()]))\n",
    "print(\"Number of clean participants:\", sum([len(df[\"worker_id\"].unique()) for df in dfs_results_no_mp.values()]))\n",
    "print(\"Number of flawed/rejected participants:\", sum([len(df[\"worker_id\"].unique()) for df in dfs_results_mp_or_rejected.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_units = dict()\n",
    "n_units = dict()\n",
    "for k in dfs_results:\n",
    "    if len(dfs_results[k]) == 0:\n",
    "        continue\n",
    "    units = set(dfs_results[k].fillna('nan').apply(lambda x: f\"{x['model'].replace('_hard99', '').replace('_hard98', '').replace('_hard95', '').replace('_hard90', '').replace('_hard85', '')}_{x['layer']}_{x['channel']}\", axis=1).to_list())\n",
    "    all_units[k] = units\n",
    "\n",
    "    n_units[k] = len(dfs_results[k].groupby([\"layer\", \"channel\"]).mean(numeric_only=True))\n",
    "\n",
    "for model, n_units in zip(*np.unique([it.split(\"_\")[0] for it in list(set([it for k in all_units for it in all_units[k]]))], return_counts=True)):\n",
    "    print(f\"Model {model}: {n_units} units\")\n",
    "\n",
    "print(\"Total number of unique units:\", len(set([it for k in all_units for it in all_units[k]])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze how many responses have been collected/accepted per unit/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs_results_main_passed\n",
    "meta_datas = []\n",
    "for k in dfs:\n",
    "    if len(dfs[k]) == 0:\n",
    "        continue\n",
    "    n_units = dfs[k].groupby([\"layer\", \"channel\"]).mean(numeric_only=True).shape[0]\n",
    "    n_responses_counts = dfs[k].groupby([\"layer\", \"channel\"])[\"correct\"].apply(lambda df: len(df)).value_counts().to_dict()\n",
    "    meta_datas.append({\n",
    "        \"model_condition\": k,\n",
    "        \"units\": n_units,\n",
    "        \"responses_per_unit\": sorted(list(n_responses_counts.keys())),\n",
    "        \"completed\": (n_responses_counts.get(31, 0) == n_responses_counts.get(32, 0) == 40 if n_units == 80 else\n",
    "            n_responses_counts.get(30, 0) == 84 if n_units == 84 else np.nan)\n",
    "        })\n",
    "meta_datas = pd.DataFrame(meta_datas)\n",
    "meta_datas = meta_datas.set_index(\"model_condition\")\n",
    "\n",
    "print(\"All Experiments\")\n",
    "display(meta_datas)\n",
    "\n",
    "print(\"Incomplete Experiments\")\n",
    "display(meta_datas[meta_datas[\"completed\"] == False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to use data of participants that participated in multiple experiments and\n",
    "# data of participants that did not pass the checks.\n",
    "use_flawed_data = False\n",
    "\n",
    "if use_flawed_data:\n",
    "    results_dir = \"results_rejected\"\n",
    "else:\n",
    "    results_dir = \"results\"\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Confidence Intervals/Stds/SEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "if use_flawed_data:\n",
    "    dfs = {k: dfs_results_main_mp_or_rejected[k].copy() for k in dfs_results_main_passed}\n",
    "else:\n",
    "    dfs = {k: dfs_results_main_passed[k].copy() for k in dfs_results_main_passed}\n",
    "\n",
    "\n",
    "dummy_test_results = collections.namedtuple(\"DummyTestResults\", (\"confidence_interval\",))((np.nan, np.nan))\n",
    "\n",
    "import scipy\n",
    "confidences = {k: scipy.stats.bootstrap(\n",
    "    dfs[k].groupby([\"layer\", \"channel\"]).mean(numeric_only=True)[\"correct\"].values.reshape(  # .groupby(\"participant_id\").mean(numeric_only=True)\n",
    "    (1, -1)), statistic=np.mean, n_resamples=1_0_000) if experiments[k] is not None else dummy_test_results for k in dfs}\n",
    "\n",
    "means = {k: dfs[k].groupby([\"layer\", \"channel\"]).mean(numeric_only=True).mean(numeric_only=True) for k in experiments}\n",
    "stds = {k: dfs[k].groupby([\"layer\", \"channel\"]).mean(numeric_only=True).std(numeric_only=True) for k in experiments}\n",
    "sems = {k: stds[k] / np.sqrt(len(dfs[k])) if len(dfs[k]) > 0 else stds[k] for k in experiments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(means, confidences, relevant_experiments, labels=(\"Natural\", \"Synthetic\"), color_names=(\"natural\", \"synthetic\"), legend: bool = True,\n",
    "                     chance_level: bool = True, xticks: bool = True, yticks: bool = True, xlabel=\"Model\", ylabel=\"Proportion Correct\",\n",
    "                     bar_width: float = 1, grid: bool = True, rotate_ticks: bool = False, y_max: float = 0.9, legend_cols: int = 2,\n",
    "                     show_errorbars: bool = True, ax=None):\n",
    "    x = np.arange(len(relevant_experiments) // len(labels) * (len(labels) + 1), dtype=float)\n",
    "\n",
    "    relevant_means = [means[k][\"correct\"] for k in relevant_experiments]\n",
    "    error_values = np.array([\n",
    "        (max(0, -confidences[k].confidence_interval[0] + means[k][\"correct\"]),\n",
    "        max(0, confidences[k].confidence_interval[1] - means[k][\"correct\"]))\n",
    "        for k in relevant_experiments]).T\n",
    "    #error_values = np.array([2 * sems[k][\"correct\"] for k in experiments])\n",
    "    #error_values = np.array([stds[k][\"correct\"] for k in experiments])\n",
    "    missing_indices = [i for i, it in enumerate(relevant_means) if np.isnan(it)]\n",
    "    #relevant_means = [it for i, it in enumerate(relevant_means) if i not in missing_indices]\n",
    "    #error_values = error_values[:, [i for i in range(len(relevant_means)) if i not in missing_indices]]\n",
    "    for i in missing_indices:\n",
    "        group_idx = i // len(labels)\n",
    "        element_idx = i % len(labels)\n",
    "        j = group_idx * (len(labels) + 1) + element_idx\n",
    "        x[j + 1:] = x[j:-1]\n",
    "\n",
    "    \n",
    "    for i in range(1, len(relevant_experiments) // len(labels)):\n",
    "        x[i * (len(labels) + 1):] -= 0.5\n",
    "\n",
    "    x = np.vstack([x[i::len(labels) + 1] for i in range(len(labels))]).reshape((-1,), order='F')\n",
    "\n",
    "    x *= bar_width\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(1 + bar_width*0.6*(len(relevant_means) - len(missing_indices)), 4.8))\n",
    "    else:\n",
    "        print(\"Reusing existing axis. Ensure that the axis has the right size:\", (1 + bar_width*0.6*(len(relevant_means) - len(missing_indices)), 4.8))\n",
    "    ax.bar(\n",
    "        x,\n",
    "        [means[k][\"correct\"] for k in relevant_experiments],\n",
    "        color=[colors[color_names[i % len(labels)].lower()] for i in range(len(relevant_experiments))],\n",
    "        width=bar_width\n",
    "        )\n",
    "    if show_errorbars:\n",
    "        ax.errorbar(x, relevant_means, \n",
    "                    error_values, fmt=\".\", color=\"k\")\n",
    "\n",
    "    if xticks:\n",
    "        single_bar_offset = bar_width * np.array([0.5 if np.isnan(means[k][\"correct\"]) else 0 for k in relevant_experiments]).reshape(-1, len(labels)).sum(-1)\n",
    "        x_ticks = (x[:-1:len(labels)] + x[len(labels) - 1::len(labels)]) / 2 - single_bar_offset\n",
    "        ax.set_xticks(x_ticks, [k.split(\"(\")[0].replace(\" \", \"\\n\") for k in relevant_experiments][::len(labels)], rotation=45 if rotate_ticks else 0)\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "\n",
    "    if not yticks:\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel, labelpad=-10)\n",
    "    ax.set_ylim((0.475, y_max))\n",
    "    \n",
    "    x_max = max([x[i] for i, k in enumerate(relevant_experiments) if not np.isnan(means[k][\"correct\"])])\n",
    "\n",
    "    if legend:\n",
    "        hdls = [ax.scatter([], [], color=colors[k.lower()]) for k in color_names]\n",
    "        ax.legend(hdls, labels, frameon=False, ncol=legend_cols)\n",
    "\n",
    "    if chance_level:\n",
    "        ax.text(x[1] + 0.6 if len(x) > 2 else x[0] - bar_width, 0.505, \"Chance\")\n",
    "        ax.hlines(0.5, x.min() - 1, x.max() + 1, color=\"k\", ls=\"-.\", lw=1)\n",
    "\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.set_xlim(-1.0 * bar_width, x_max + 0.75 * bar_width)\n",
    "    ax.spines[\"bottom\"].set_bounds((-0.75 * bar_width, x_max + 0.75 * bar_width))\n",
    "\n",
    "    if grid:\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(linestyle=\"dashed\", axis=\"y\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "relevant_experiments = [k for k in experiments if \"hard\" not in k.lower()]\n",
    "plot_performance(means, confidences, relevant_experiments, bar_width=0.6)\n",
    "plt.xlabel(\"Model (Ordered by Scale)\")\n",
    "plt.savefig(os.path.join(results_dir, \"model_comparison.pdf\"), bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_experiments = [k for k in experiments if \"hard\" not in k.lower() and \"natural\" in k.lower()]\n",
    "print(len(relevant_experiments), len(means))\n",
    "plot_performance(means, confidences, relevant_experiments, labels=[\"\"] * len(relevant_experiments),\n",
    "                 color_names=[f\"c{i}\" for i in range(len(relevant_experiments))],\n",
    "                 legend=False, chance_level=False,\n",
    "                 xticks=False, yticks=True, xlabel=\"Model & Dataset Size\", ylabel=\"Interpretability Score\", bar_width=0.4, grid=False,\n",
    "                 show_errorbars=False)\n",
    "plt.gcf().set_size_inches(2, 2)\n",
    "plt.savefig(os.path.join(results_dir, \"comparison_figure_1.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyInterval:\n",
    "    confidence_interval = (0, 0)\n",
    "relevant_experiments = [k for k in experiments if \"hard\" not in k.lower()]\n",
    "\n",
    "selector_fn_0 = lambda k: False\n",
    "selector_fn_1 = lambda k: \"GoogLeNet (natural)\" == k\n",
    "selector_fn_2 = lambda k: \"natural\" in k\n",
    "\n",
    "for i, selector_fn in enumerate((selector_fn_0, selector_fn_1, selector_fn_2)):\n",
    "    plot_performance(\n",
    "        {k: means[k] if selector_fn(k) else means[k] * 0 for k in means},\n",
    "        {k: confidences[k] if selector_fn(k) else DummyInterval() for k in confidences},\n",
    "        relevant_experiments, bar_width=0.6, legend=i>1)\n",
    "    plt.xlabel(\"Model (Ordered by Scale)\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"model_comparison_slides_variation_{i + 1}.pdf\"), bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard vs. Easy Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_experiments = [\n",
    "    \"ResNet (natural)\",\n",
    "    \"Hard99 ResNet (natural)\",\n",
    "    \"Hard95 ResNet (natural)\",\n",
    "    \"Hard85 ResNet (natural)\",\n",
    "    \"Clip ResNet (natural)\",\n",
    "    \"Hard95 Clip ResNet (natural)\",\n",
    "    \"Hard99 Clip ResNet (natural)\",\n",
    "    \"Hard85 Clip ResNet (natural)\",\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(3.88+1.72, 4.8), sharey=True, width_ratios=[3.88, 1.12])\n",
    "plot_performance(means, confidences, relevant_experiments, (\"Easy\", \"Medium\", \"Hard\", \"Very Hard\"),\n",
    "                 color_names=(\"natural easy\", \"natural medium\", \"natural hard\", \"natural very hard\"),  bar_width=0.6,\n",
    "                 ax=axs[0])\n",
    "\n",
    "# Uncomment to get plots separately.\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"results/model_comparison_rn_easy_vs_hard_natural.pdf\", bbox_inches=\"tight\")\n",
    "# plt.show()\n",
    "\n",
    "relevant_experiments = [\n",
    "    \"ResNet (synthetic)\",\n",
    "    \"Hard95 ResNet (synthetic)\",\n",
    "]\n",
    "plot_performance(means, confidences, relevant_experiments, (\"Easy\", \"Hard\"),\n",
    "                 color_names=(\"synthetic easy\", \"synthetic medium\"), bar_width=0.6, legend_cols=1,\n",
    "                 ax=axs[1])\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# Uncomment to get plots separately.\n",
    "#plt.savefig(os.path.join(results_dir, \"model_comparison_rn_easy_vs_hard_optimized.pdf\"), bbox_inches=\"tight\")\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, \"model_comparison_rn_easy_vs_hard_natural_and_optimized.pdf\"), bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_experiments = [k for k in experiments if \"hard\" not in k.lower()]\n",
    "\n",
    "if use_flawed_data:\n",
    "    dfs = {k: dfs_results_main_mp_or_rejected[k].copy() for k in relevant_experiments}\n",
    "else:\n",
    "    dfs = {k: dfs_results_main_passed[k].copy() for k in relevant_experiments}\n",
    "\n",
    "unit_mean_dfs = {k: dfs[k].groupby([\"layer\", \"channel\"]).mean(numeric_only=True).reset_index() for k in relevant_experiments}\n",
    "\n",
    "model_names = {\n",
    "    \"ResNet\": \"resnet50\",\n",
    "    \"Robust ResNet\": \"resnet50-l2\",\n",
    "    \"GoogLeNet\": \"googlenet\",\n",
    "    \"Clip ResNet\": \"clip-resnet50\",\n",
    "    \"WideResNet\": \"wide_resnet50\",\n",
    "    \"DenseNet\": \"densenet_201\",\n",
    "    \"ConvNeXT\": \"convnext_b\",\n",
    "    \"Clip ViT\": \"clip-vit_b32\",\n",
    "    \"ViT\": \"in1k-vit_b32\",\n",
    "    \"Hard99 Clip ResNet\": \"clip-resnet50\",\n",
    "    \"Hard99 ResNet\": \"resnet50\",\n",
    "    \"Hard95 Clip ResNet\": \"clip-resnet50\",\n",
    "    \"Hard95 ResNet\": \"resnet50\",\n",
    "    \"Hard85 Clip ResNet\": \"clip-resnet50\",\n",
    "    \"Hard85 ResNet\": \"resnet50\",\n",
    "}\n",
    "\n",
    "for i, k in enumerate(list(unit_mean_dfs.keys())):\n",
    "    model_name = model_names[k.split(\" (\")[0]]\n",
    "    model = utils_stimuli_generation.load_model(model_name)\n",
    "    network_layers = utils_stimuli_generation.get_relevant_layers(model, model_name)\n",
    "\n",
    "    if model_name == \"clip-vit_b32\":\n",
    "        network_layers = [\"visual_\" + it for it in network_layers]\n",
    "\n",
    "    unit_mean_dfs[k][\"layer_index\"] = unit_mean_dfs[k][\"layer\"].map(lambda l: network_layers.index(l))\n",
    "\n",
    "    unit_mean_dfs[k] = unit_mean_dfs[k].sort_values(\"layer_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('tab10')\n",
    "\n",
    "relevant_experiments = [k for k in experiments if \"natural\" in k.lower() and \"hard\" not in k.lower()]\n",
    "\n",
    "color_values = [cmap(i / len(relevant_experiments)) for i in range(len(relevant_experiments))]\n",
    "\n",
    "relevant_unit_mean_dfs = {k: unit_mean_dfs[k] for k in relevant_experiments}\n",
    "\n",
    "for i, k in enumerate(relevant_experiments):\n",
    "    print(k, np.sum(relevant_unit_mean_dfs[k][\"correct\"] > 0.95), len(relevant_unit_mean_dfs[k][\"correct\"]))\n",
    "\n",
    "for i, k in enumerate(relevant_experiments):\n",
    "    label = k.split(\" (\")[0]\n",
    "\n",
    "    # Pool units in the same layer and compute mean proportion correct.\n",
    "    grouped_df = relevant_unit_mean_dfs[k].groupby(\"layer_index\").apply(lambda df: df[[\"layer_index\", \"correct\"]].mean())\n",
    "\n",
    "    x = grouped_df[\"layer_index\"] / grouped_df[\"layer_index\"].max()\n",
    "    y = grouped_df[\"correct\"]\n",
    "\n",
    "    ax.scatter(x, y,# + np.random.uniform(-0.01, 0.01, size=len(unit_mean_dfs[k])),\n",
    "               s=6, label=label, color=color_values[i])\n",
    "\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.set_ylabel(\"Proportion Correct\")\n",
    "ax.set_xlabel(\"Relative Layer Index\")\n",
    "ax.legend(ncol=3)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(int(np.ceil(len(relevant_unit_mean_dfs) / 3)), 3, sharex=False, sharey=False)\n",
    "fig.set_size_inches(6, 5)\n",
    "\n",
    "axs_f = axs.flatten()\n",
    "for ax in axs_f:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for i, (k, ax) in enumerate(zip(relevant_unit_mean_dfs, axs_f)):\n",
    "    label = k.split(\" (\")[0]\n",
    "\n",
    "    # Pool units in the same layer and compute mean proportion correct.\n",
    "    grouped_df = relevant_unit_mean_dfs[k].groupby(\"layer_index\").apply(lambda df: df[[\"layer_index\", \"correct\"]].mean())\n",
    "\n",
    "    x = grouped_df[\"layer_index\"] / grouped_df[\"layer_index\"].max()\n",
    "    y = grouped_df[\"correct\"]\n",
    "    ax.scatter(\n",
    "        x, y,\n",
    "        s=7,\n",
    "        color=color_values[i],\n",
    "        alpha=0.8,\n",
    "        linewidth=0,\n",
    "        clip_on=False\n",
    "    )\n",
    "\n",
    "    print(f\"{k}\\t\", scipy.stats.spearmanr(x, y))\n",
    "    ax.set_title(label)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_bounds((0, 1))\n",
    "    ax.spines[\"left\"].set_bounds((0.25, 1))\n",
    "    ax.set_ylim((0.25, 1))\n",
    "    ax.axis(\"on\")\n",
    "\n",
    "    ax.tick_params(axis='both', which='both', labelsize=11)\n",
    "\n",
    "axs[1, 0].set_ylabel(\"Proportion Correct (Natural)\", fontsize=12)\n",
    "axs[-1, 1].set_xlabel(\"Relative Layer Position\", fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axs_f):\n",
    "    if i == 0 or i == 3 or i == 6:\n",
    "        ax.set_yticks([0.25, 0.5, 0.75, 1.0], [\"0.25\", \"0.50\", \"0.75\", \"1.0\"])\n",
    "    else:\n",
    "        ax.set_yticks([0.25, 0.5, 0.75, 1.0], [\"\", \"\", \"\", \"\"])\n",
    "    if i == 6 or i == 7 or i == 8:\n",
    "        ax.set_xticks([0.0, 0.5, 1.0], [\"0.0\", \"0.5\", \"1.0\"])\n",
    "    else:\n",
    "        ax.set_xticks([0.0, 0.5, 1.0], [\"\", \"\", \"\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, \"model_comparison_unit_performance.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('tab10')\n",
    "\n",
    "relevant_synthetic_experiments = [k for k in experiments if \"synthetic\" in k.lower() and not \"hard\" in k.lower() if len(unit_mean_dfs[k]) > 0]\n",
    "relevant_natural_experiments = [k.replace(\"synthetic\", \"natural\") for k in relevant_synthetic_experiments]\n",
    "relevant_natural_experiments = [k for k in relevant_natural_experiments if k in experiments]\n",
    "\n",
    "#color_values = [cmap(i / (len(relevant_synthetic_experiments) - 1)) for i in range(len(relevant_synthetic_experiments))]\n",
    "\n",
    "fig, axs = plt.subplots(int(np.ceil(len(relevant_synthetic_experiments) / 3)), 3, sharex=False, sharey=False)\n",
    "fig.set_size_inches(6, 5)\n",
    "\n",
    "axs_f = axs.flatten()\n",
    "for ax in axs_f:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "if len(relevant_natural_experiments) == 7:\n",
    "    relevant_natural_experiments = relevant_natural_experiments[:6] + [None, relevant_natural_experiments[-1]]\n",
    "    relevant_synthetic_experiments = relevant_synthetic_experiments[:6] + [None, relevant_synthetic_experiments[-1]]\n",
    "\n",
    "i_correction = 0\n",
    "for i, (k_natural, k_synthetic, ax) in enumerate(zip(relevant_natural_experiments, relevant_synthetic_experiments, axs_f)):\n",
    "    if k_natural is None:\n",
    "        i_correction += 1\n",
    "        continue\n",
    "    label = k_natural.split(\" (\")[0]\n",
    "\n",
    "    grouped_natural_df = unit_mean_dfs[k_natural].set_index([\"layer\", \"channel\"])\n",
    "    grouped_synthetic_df = unit_mean_dfs[k_synthetic].set_index([\"layer\", \"channel\"])\n",
    "\n",
    "    x = grouped_natural_df[\"correct\"]\n",
    "    y = grouped_synthetic_df[\"correct\"]\n",
    "\n",
    "    print(f\"{label}\\t{x.mean()} {y.mean()}\")\n",
    "\n",
    "    ax.scatter(\n",
    "        x, y,\n",
    "        s=7,\n",
    "        color=color_values[i + i_correction],\n",
    "        alpha=0.8,\n",
    "        linewidth=0,\n",
    "        clip_on=False\n",
    "    )\n",
    "\n",
    "    # ax.scatter([x.mean()], [y.mean()], color=\"red\")\n",
    "\n",
    "    print(f\"{label}\\t\", scipy.stats.spearmanr(x, y))\n",
    "    ax.set_title(label)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_bounds((0.40, 1))\n",
    "    ax.spines[\"left\"].set_bounds((0.00, 1))\n",
    "    ax.set_ylim((0.00, 1))\n",
    "    ax.set_xlim((0.35, 1))\n",
    "    ax.axis(\"on\")\n",
    "\n",
    "    ax.tick_params(axis='both', which='both', labelsize=11)\n",
    "\n",
    "for i, ax in enumerate(axs_f):\n",
    "    if not (i == 0 or i == 3):\n",
    "        ax.set_yticks([0, 0.5, 1.0], [\"\", \"\", \"\"])\n",
    "    else:\n",
    "        ax.set_yticks([0, 0.5, 1.0], [\"0.0\", \"0.5\", \"1.0\"])\n",
    "    if i == 3 or i == 5 or i == 7:\n",
    "        ax.set_xticks([0.4, 0.7, 1.0], [\"0.4\", \"0.7\", \"1.0\"])\n",
    "    else:\n",
    "        ax.set_xticks([0.4, 0.7, 1.0], [\"\", \"\", \"\"])\n",
    "\n",
    "\n",
    "axs[1, 0].set_ylabel(\"Proportion Correct (Synthetic)\", fontsize=12)\n",
    "axs[-1, 1].set_xlabel(\"Proportion Correct (Natural)\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "axs_f[-2].set_autoscale_on(False)\n",
    "# Adding yticklabels for last plot manually\n",
    "ticks = [0.0, 0.5, 1.0]\n",
    "for tick in ticks:\n",
    "    axs_f[-2].text(\n",
    "        x=-0.225,\n",
    "        y=tick - 0.05, # small offset for some reason\n",
    "        s=str(tick),\n",
    "        transform=axs_f[-2].transAxes,\n",
    "        fontsize=11\n",
    "    )\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, \"model_comparison_unit_performance_natural_vs_synthetic.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pool units in the same layer and compute mean proportion correct.\n",
    "k = \"Clip ResNet (natural)\"\n",
    "grouped_df = unit_mean_dfs[k].groupby(\"layer_index\").apply(lambda df: df[[\"layer_index\", \"correct\"]].mean())\n",
    "\n",
    "grouped_df = grouped_df[grouped_df[\"correct\"] <0.90]\n",
    "\n",
    "x = grouped_df[\"layer_index\"] / grouped_df[\"layer_index\"].max()\n",
    "y = grouped_df[\"correct\"]\n",
    "print(f\"{k}\\t\", scipy.stats.spearmanr(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_experiments = list(experiments.keys())\n",
    "\n",
    "if use_flawed_data:\n",
    "    dfs = {k: dfs_results_main_mp_or_rejected[k].copy() for k in relevant_experiments}\n",
    "else:\n",
    "    dfs = {k: dfs_results_main_passed[k].copy() for k in relevant_experiments}\n",
    "unit_mean_dfs = {k: dfs[k].groupby([\"layer\", \"channel\"]).mean(numeric_only=True).reset_index() for k in relevant_experiments}\n",
    "\n",
    "joined_easy_hard_dfs = {}\n",
    "\n",
    "easy_hard_condition_names = {\n",
    "    #\"ResNet50 (natural)\": (\"Hard85 ResNet (natural)\", \"Hard95 ResNet (natural)\", \"Hard99 ResNet (natural)\", \"ResNet (natural)\"),\n",
    "    #\"Clip ResNet50 (natural)\": (\"Hard85 Clip ResNet (natural)\", \"Hard95 Clip ResNet (natural)\", \"Hard99 Clip ResNet (natural)\", \"Clip ResNet (natural)\")\n",
    "    \"ResNet50 (natural)\": (\"Hard95 ResNet (natural)\", \"Hard99 ResNet (natural)\", \"ResNet (natural)\"),\n",
    "    \"Clip ResNet50 (natural)\": (\"Hard95 Clip ResNet (natural)\", \"Hard99 Clip ResNet (natural)\", \"Clip ResNet (natural)\")\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(1, len(easy_hard_condition_names), sharey=True)\n",
    "fig.set_size_inches(2+4*len(easy_hard_condition_names), 4)\n",
    "\n",
    "for ax, mk in zip(axs, easy_hard_condition_names):\n",
    "    restricted_unit_mean_dfs = {}\n",
    "    for k in easy_hard_condition_names[mk]:\n",
    "        restricted_unit_mean_dfs[k] = unit_mean_dfs[k].copy()[[\"layer\", \"channel\", \"correct\"]]\n",
    "        restricted_unit_mean_dfs[k][\"layer_channel\"] = restricted_unit_mean_dfs[k].apply(lambda row: f\"{row['layer']}:{row['channel']}\", axis=1)\n",
    "        restricted_unit_mean_dfs[k] = restricted_unit_mean_dfs[k].sort_values(\"layer_channel\")\n",
    "\n",
    "    shared_units = set.intersection(*[set(restricted_unit_mean_dfs[k][\"layer_channel\"].to_list()) for k in easy_hard_condition_names[mk]])\n",
    "    for k in easy_hard_condition_names[mk]:\n",
    "        model_name = model_names[k.split(\" (\")[0]]\n",
    "        model = utils_stimuli_generation.load_model(model_name)\n",
    "        network_layers = utils_stimuli_generation.get_relevant_layers(model, model_name)\n",
    "        restricted_unit_mean_dfs[k] = restricted_unit_mean_dfs[k][restricted_unit_mean_dfs[k][\"layer_channel\"].map(lambda lc: lc in shared_units)]\n",
    "        restricted_unit_mean_dfs[k][\"layer_index\"] = restricted_unit_mean_dfs[k][\"layer\"].map(lambda l: network_layers.index(l))\n",
    "        restricted_unit_mean_dfs[k][\"layer_unit_index\"] = np.argsort(restricted_unit_mean_dfs[k].apply(lambda row: row[\"layer_index\"] + int(row[\"channel\"]) / 10000, axis=1))\n",
    "        restricted_unit_mean_dfs[k][\"relative_layer_unit_index\"] = restricted_unit_mean_dfs[k][\"layer_unit_index\"] / restricted_unit_mean_dfs[k][\"layer_unit_index\"].max()\n",
    "\n",
    "        restricted_unit_mean_dfs[k] = restricted_unit_mean_dfs[k].set_index([\"layer\", \"channel\"])\n",
    "\n",
    "        restricted_unit_mean_dfs[k].columns = [f\"{'hard95' if 'Hard95' in k else 'hard85' if 'Hard85' in k else 'hard99' if 'Hard99' in k else 'easy'}_{c}\" for c in restricted_unit_mean_dfs[k].columns]\n",
    "\n",
    "    joined_easy_hard_dfs[mk] = pd.concat(restricted_unit_mean_dfs.values(), ignore_index=False, axis=1)\n",
    "\n",
    "    from matplotlib.collections import LineCollection\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    if len(easy_hard_condition_names[mk]) == 4:\n",
    "        column_names = (\"hard85_correct\", \"hard95_correct\", \"hard99_correct\", \"easy_correct\")\n",
    "    elif len(easy_hard_condition_names[mk]) == 3:\n",
    "        column_names = (\"hard95_correct\", \"hard99_correct\", \"easy_correct\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid number of easy/hard conditions. Found {len(easy_hard_condition_names[mk])} but expected 3 or 4.\")\n",
    "\n",
    "    if \"hard85_relative_layer_unit_index\" in joined_easy_hard_dfs[mk]:\n",
    "        x = joined_easy_hard_dfs[mk][\"hard85_relative_layer_unit_index\"]\n",
    "    elif \"hard95_relative_layer_unit_index\" in joined_easy_hard_dfs[mk]:\n",
    "        x = joined_easy_hard_dfs[mk][\"hard95_relative_layer_unit_index\"]\n",
    "    elif \"hard99_relative_layer_unit_index\" in joined_easy_hard_dfs[mk]:\n",
    "        x = joined_easy_hard_dfs[mk][\"hard95_relative_layer_unit_index\"]\n",
    "    else:\n",
    "        x = joined_easy_hard_dfs[mk][\"easy_relative_layer_unit_index\"]\n",
    "    min_y = joined_easy_hard_dfs[mk].apply(lambda row: min([row[k] for k in column_names]), axis=1)\n",
    "    max_y = joined_easy_hard_dfs[mk].apply(lambda row: max([row[k] for k in column_names]), axis=1)\n",
    "\n",
    "    [\n",
    "        (247/255, 125/255, 40/255),\n",
    "        (51/255, 120/255, 177/255),\n",
    "        (44/255, 160/255, 44/255),\n",
    "        (0, 0, 0),\n",
    "    ]\n",
    "    colors_raw = [\n",
    "      {\n",
    "        \"hard85\": (247/255, 125/255, 40/255),\n",
    "        \"hard95\": (51/255, 120/255, 177/255),\n",
    "        \"hard99\": (44/255, 160/255, 44/255),\n",
    "        \"easy\": (0, 0, 0)\n",
    "    }[k.split(\"_\")[0]] for k in column_names]\n",
    "\n",
    "    for i in range(len(min_y)):\n",
    "        ys = np.linspace(min_y[i], max_y[i], 100)\n",
    "        zs = np.linspace(min_y[i], max_y[i], 100)\n",
    "        points = np.array([[(x[i], ys[j]) for j in range(len(ys))]]).reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "        \n",
    "        y_raw = [joined_easy_hard_dfs[mk][k][i] for k in column_names]\n",
    "        sorted_idxs = np.argsort(y_raw)\n",
    "        y_sorted = [y_raw[i] for i in sorted_idxs]\n",
    "        colors_sorted = [colors_raw[i] for i in sorted_idxs]\n",
    "\n",
    "        cmap_list = [(0, (0, 0, 0))] + [(y_sorted[j], colors_sorted[j]) for j in range(3)] + [(1, (0, 0, 0))]\n",
    "        cmap = LinearSegmentedColormap.from_list(f\"interpolation-{mk}-{i}\", colors=cmap_list)\n",
    "        lc = LineCollection(segments, array=zs, cmap=cmap, norm=plt.Normalize(0, 1), alpha=0.5)\n",
    "        #lc.set_array(zs)\n",
    "        ax.add_collection(lc)\n",
    "\n",
    "    if len(colors_raw) == 4:\n",
    "        legend_items = [(\"black\", \"v\", \"Easy\"), (\"C2\", \"o\", \"Medium\"), (\"C0\", \"x\", \"Hard\"), (\"C1\", \"^\", \"Very Hard\")]\n",
    "        scatter_items = [(\"black\", \"v\", \"easy_correct\"), (\"C2\", \"o\", \"hard99_correct\"), (\"C0\", \"x\", \"hard95_correct\"), (\"C1\", \"^\", \"hard85_correct\")]\n",
    "    else:\n",
    "        legend_items = [(\"black\", \"v\", \"Easy\"), (\"C0\", \"x\", \"Medium\"), (\"C1\", \"^\", \"Hard\")]\n",
    "        scatter_items = [(\"black\", \"v\", \"easy_correct\"), (\"C0\", \"x\", \"hard99_correct\"), (\"C1\", \"^\", \"hard95_correct\")]\n",
    "\n",
    "    for color, m, k in scatter_items:\n",
    "        ax.scatter(joined_easy_hard_dfs[mk][k.replace(\"correct\", \"relative_layer_unit_index\")], joined_easy_hard_dfs[mk][k],\n",
    "                s=8, color=color, zorder=2, marker=m)\n",
    "\n",
    "    ax.set_ylabel(\"Proportion Correct\")\n",
    "    ax.set_xlabel(\"Relative Layer Position\")\n",
    "\n",
    "    ax.set_title(label)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    ax.spines[\"bottom\"].set_bounds(0, 1)\n",
    "    ax.spines[\"left\"].set_bounds(0.3, 1)\n",
    "\n",
    "    ax.set_title(mk)\n",
    "\n",
    "    \n",
    "    ax.legend([ax.scatter([], [], c=c, s=10, marker=m) for (c, m, _) in legend_items], [it[-1] for it in legend_items], frameon=False, ncol=len(legend_items))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/model_comparison_units_rn_easy_vs_hard.pdf\", bbox_inches=\"tight\")\n",
    "#for k in joined_easy_hard_dfs:\n",
    "#    print(k)\n",
    "#    display(joined_easy_hard_dfs[k].sort_values(\"correct_gap\")[[\"correct_gap\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"DejaVu Sans\"]\n",
    "\n",
    "relevant_experiments = list(experiments.keys())\n",
    "\n",
    "if use_flawed_data:\n",
    "    dfs = {k: dfs_results_main_mp_or_rejected[k].copy() for k in relevant_experiments}\n",
    "else:\n",
    "    dfs = {k: dfs_results_main_passed[k].copy() for k in relevant_experiments}\n",
    "unit_mean_dfs = {k: dfs[k].groupby([\"layer\", \"channel\"]).mean(numeric_only=True).reset_index() for k in relevant_experiments}\n",
    "\n",
    "joined_easy_hard_dfs = {}\n",
    "\n",
    "easy_hard_condition_names = {\n",
    "    \"ResNet\": (\"Hard85 ResNet (natural)\", \"Hard95 ResNet (natural)\", \"ResNet (natural)\"),\n",
    "    \"Clip ResNet\": (\"Hard85 Clip ResNet (natural)\", \"Hard95 Clip ResNet (natural)\", \"Clip ResNet (natural)\")\n",
    "}\n",
    "\n",
    "fig, axss = plt.subplots(len(easy_hard_condition_names), 4, sharex=False, sharey=True)\n",
    "fig.set_size_inches(6*2, 2*len(easy_hard_condition_names))\n",
    "\n",
    "for i, (axs, mk) in enumerate(zip(axss, easy_hard_condition_names)):\n",
    "    restricted_unit_mean_dfs = {}\n",
    "    for k in easy_hard_condition_names[mk]:\n",
    "        restricted_unit_mean_dfs[k] = unit_mean_dfs[k].copy()[[\"layer\", \"channel\", \"correct\"]]\n",
    "        restricted_unit_mean_dfs[k][\"layer_channel\"] = restricted_unit_mean_dfs[k].apply(lambda row: f\"{row['layer']}:{row['channel']}\", axis=1)\n",
    "        restricted_unit_mean_dfs[k] = restricted_unit_mean_dfs[k].sort_values(\"layer_channel\")\n",
    "\n",
    "    shared_units = set.intersection(*[set(restricted_unit_mean_dfs[k][\"layer_channel\"].to_list()) for k in easy_hard_condition_names[mk]])\n",
    "    for k in easy_hard_condition_names[mk]:\n",
    "        model_name = model_names[k.split(\" (\")[0]]\n",
    "        model = utils_stimuli_generation.load_model(model_name)\n",
    "        network_layers = utils_stimuli_generation.get_relevant_layers(model, model_name)\n",
    "        restricted_unit_mean_dfs[k] = restricted_unit_mean_dfs[k][restricted_unit_mean_dfs[k][\"layer_channel\"].map(lambda lc: lc in shared_units)]\n",
    "        restricted_unit_mean_dfs[k][\"layer_index\"] = restricted_unit_mean_dfs[k][\"layer\"].map(lambda l: network_layers.index(l))\n",
    "        restricted_unit_mean_dfs[k][\"layer_unit_index\"] = np.argsort(restricted_unit_mean_dfs[k].apply(lambda row: row[\"layer_index\"] + int(row[\"channel\"]) / 10000, axis=1))\n",
    "        restricted_unit_mean_dfs[k][\"relative_layer_unit_index\"] = restricted_unit_mean_dfs[k][\"layer_unit_index\"] / restricted_unit_mean_dfs[k][\"layer_unit_index\"].max()\n",
    "\n",
    "        restricted_unit_mean_dfs[k] = restricted_unit_mean_dfs[k].set_index([\"layer\", \"channel\"])\n",
    "\n",
    "        restricted_unit_mean_dfs[k].columns = [f\"{'hard95' if 'Hard95' in k else 'hard85' if 'Hard85' in k else 'easy'}_{c}\" for c in restricted_unit_mean_dfs[k].columns]\n",
    "\n",
    "    joined_easy_hard_dfs[mk] = pd.concat(restricted_unit_mean_dfs.values(), ignore_index=False, axis=1)\n",
    "\n",
    "    joined_easy_hard_dfs[mk][\"relative_easy_correct\"] = joined_easy_hard_dfs[mk][\"easy_correct\"] - joined_easy_hard_dfs[mk][\"easy_correct\"].min()\n",
    "    joined_easy_hard_dfs[mk][\"relative_easy_correct\"] = joined_easy_hard_dfs[mk][\"relative_easy_correct\"] / joined_easy_hard_dfs[mk][\"relative_easy_correct\"].max()\n",
    "\n",
    "    joined_easy_hard_dfs[mk][\"correct_gap_85\"] = joined_easy_hard_dfs[mk][\"easy_correct\"] - joined_easy_hard_dfs[mk][\"hard85_correct\"]\n",
    "    joined_easy_hard_dfs[mk][\"correct_gap_95\"] = joined_easy_hard_dfs[mk][\"easy_correct\"] - joined_easy_hard_dfs[mk][\"hard95_correct\"]\n",
    "\n",
    "    df = joined_easy_hard_dfs[mk].sort_values(\"hard85_relative_layer_unit_index\")\n",
    "    axs[0].plot(df[\"hard85_relative_layer_unit_index\"], df[\"correct_gap_85\"], ls=\":\", alpha=0.5)\n",
    "    axs[0].scatter(df[\"hard85_relative_layer_unit_index\"], df[\"correct_gap_85\"], s=5)\n",
    "    df = joined_easy_hard_dfs[mk].sort_values(\"hard95_relative_layer_unit_index\")\n",
    "    axs[2].plot(df[\"hard95_relative_layer_unit_index\"], df[\"correct_gap_95\"], ls=\":\", alpha=0.5)\n",
    "    axs[2].scatter(df[\"hard95_relative_layer_unit_index\"], df[\"correct_gap_95\"], s=5)\n",
    "    df = joined_easy_hard_dfs[mk].sort_values(\"easy_correct\")\n",
    "    axs[1].plot(df[\"easy_correct\"], df[\"correct_gap_85\"], ls=\":\", alpha=0.5)\n",
    "    axs[3].plot(df[\"easy_correct\"], df[\"correct_gap_95\"], ls=\":\", alpha=0.5)\n",
    "    axs[1].scatter(df[\"easy_correct\"], df[\"correct_gap_85\"], s=5)\n",
    "    axs[3].scatter(df[\"easy_correct\"], df[\"correct_gap_95\"], s=5)\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis=\"both\", which=\"both\", labelsize=11)\n",
    "\n",
    "    if i == len(axss) - 1:\n",
    "        axs[0].set_xlabel(\"Relative Layer Position\", fontsize=12)\n",
    "        axs[2].set_xlabel(\"Relative Layer Position\", fontsize=12)\n",
    "\n",
    "        axs[1].set_xlabel(\"Proportion Correct (Easy)\", fontsize=12)\n",
    "        axs[3].set_xlabel(\"Proportion Correct (Easy)\", fontsize=12)\n",
    "\n",
    "    if i != len(axss) - 1:\n",
    "        for ax in axs:\n",
    "            ax.set_xticklabels([])\n",
    "\n",
    "    for j in (1, 3):\n",
    "        axs[j].set_xlim(0.43, 1.05)\n",
    "        axs[j].spines[\"bottom\"].set_bounds(0.48, 1)\n",
    "        #ax.set_yticklabels([])\n",
    "\n",
    "    axs[0].set_ylabel(\"$\\quad$\")\n",
    "    axs[2].set_ylabel(\"$\\quad$\")\n",
    "\n",
    "    for j in (0, 2):\n",
    "        axs[j].set_xlim(-0.05, 1.05)\n",
    "        axs[j].spines[\"bottom\"].set_bounds(0.0, 1)\n",
    "\n",
    "    #ax.set_title(label)\n",
    "    for ax in axs:\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    fig.text(-0.0085, 1.0 - (i+0.5)/len(axss) + (0.1 if i == 1 else 0), mk, va='center', rotation='vertical', size=12)\n",
    "\n",
    "fig.text(0.015, 0.55, 'Proportion Correct Gap (Easy → Medium)', va='center', rotation='vertical', size=12)\n",
    "fig.text(0.52, 0.55, 'Proportion Correct Gap (Easy → Hard)', va='center', rotation='vertical', size=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"model_comparison_units_rn_easy_vs_hard_gaps.pdf\"), bbox_inches=\"tight\")\n",
    "#for k in joined_easy_hard_dfs:\n",
    "#    print(k)\n",
    "#    display(joined_easy_hard_dfs[k].sort_values(\"correct_gap\")[[\"correct_gap\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in (\"natural\", \"synthetic\"):\n",
    "    relevant_experiments = [k for k in experiments if mode in k.lower() and \"hard\" not in k.lower() and len(dfs_results_main_passed[k]) > 0]\n",
    "\n",
    "    dfs = dfs_results_main_passed\n",
    "\n",
    "    means_conditioned_on_confidence = {}\n",
    "    confidences_conditioned_on_confidence = {}\n",
    "    for k in relevant_experiments:\n",
    "        for c in (1, 3):\n",
    "            confidences_conditioned_on_confidence[f\"{k}_{c}\"] = scipy.stats.bootstrap(\n",
    "                dfs[k][dfs[k][\"confidence\"] == c].groupby([\"layer\", \"channel\"]).mean(numeric_only=True)[\"correct\"].values.reshape(\n",
    "                (1, -1)), statistic=np.mean, n_resamples=10_000) if experiments[k] is not None else dummy_test_results\n",
    "\n",
    "            means_conditioned_on_confidence[f\"{k}_{c}\"] = dfs[k][dfs[k][\"confidence\"] == c].groupby([\"layer\", \"channel\"]).mean(numeric_only=True).mean(numeric_only=True)\n",
    "            \n",
    "    plot_performance(means_conditioned_on_confidence, confidences_conditioned_on_confidence, list(means_conditioned_on_confidence.keys()),\n",
    "                     (\"Low Confidence\", \"High Confidence\"), (f\"{mode.capitalize()} Low\", f\"{mode.capitalize()} High\"), bar_width=0.6, y_max=0.925,\n",
    "                     legend_cols=1)\n",
    "    plt.xlabel(\"Model (Ordered by Scale)\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"model_comparison_{mode}_confidence.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stacked_confidence(means, relevant_experiments, labels=(\"Low\", \"Medium\", \"High\", \"Low\", \"Medium\", \"High\"),\n",
    "                            legend_labels=(\"Confidence (Natural)\", \"Confidence (Synthetic)\", \"Low\", \"Low\", \"Medium\", \"Medium\", \"High\", \"High\"),\n",
    "                            color_names=(\"natural easy\", \"natural medium\", \"natural hard\", \"synthetic easy\", \"synthetic medium\", \"synthetic hard\"), legend: bool = True,\n",
    "                            legend_color_names=(None, None, \"natural easy\", \"synthetic easy\", \"natural medium\", \"synthetic medium\", \"natural hard\", \"synthetic hard\"),\n",
    "\n",
    "                     xticks: bool = True, yticks: bool = True, xlabel=\"Model\", ylabel=\"Relative Number of Responses\",\n",
    "                     bar_width: float = 1, grid: bool = True, rotate_ticks: bool = False, y_max: float = 0.9, legend_cols: int = 2):\n",
    "    # x = np.arange(len(relevant_experiments) // len(labels) * 2, dtype=float)\n",
    "\n",
    "    relevant_means = [means[k][\"count\"] for k in relevant_experiments]\n",
    "    missing_indices = [i for i, it in enumerate(relevant_means) if np.isnan(it)]\n",
    "    missing_indices = sorted(missing_indices, reverse=True)\n",
    "\n",
    "    x = []\n",
    "    color_values = []\n",
    "    idx_correction = 0\n",
    "    for i in range(0, len(relevant_experiments) // 3):\n",
    "        # for gaps between pairs of pairs of bars\n",
    "        if i * 3 in missing_indices:\n",
    "            idx_correction -= 1\n",
    "        else:\n",
    "            x += [i + idx_correction]\n",
    "            for j in range(3):\n",
    "                color_values += [colors[color_names[3 * (i % 2) + j].lower()]]\n",
    "        if i % 2 == 1:\n",
    "            idx_correction += 0.5\n",
    "        \n",
    "    x = np.array(x, dtype=float)\n",
    "    x *= bar_width\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(1 + bar_width*0.6*(len(relevant_means) // 3), 4.8))\n",
    "\n",
    "    #x = np.arange(len(bottom))\n",
    "    bottom = None\n",
    "    for ci in range(3):\n",
    "        y = np.array([means[k][\"count\"] for k in relevant_experiments])\n",
    "        y = np.array([y[i] for i in range(len(y)) if not i in missing_indices])\n",
    "\n",
    "        y = y[ci::3]\n",
    "\n",
    "        if bottom is None:\n",
    "            bottom = np.zeros_like(y)\n",
    "\n",
    "        ax.bar(\n",
    "            x,\n",
    "            y,\n",
    "            color=color_values[ci::3],\n",
    "            width=bar_width,\n",
    "            bottom=bottom\n",
    "            )\n",
    "        bottom += y\n",
    "\n",
    "    if xticks:\n",
    "        #single_bar_offset = bar_width * np.array([0.5 if np.isnan(means[k][\"correct\"]) else 0 for k in relevant_experiments]).reshape(-1, len(labels)).sum(-1)\n",
    "        x_ticks = []\n",
    "        x_tick_labels = []\n",
    "        for i in range(0, len(relevant_experiments) // 3):\n",
    "            if i * 3 not in missing_indices:\n",
    "                x_tick_labels += [relevant_experiments[3 * i].split(\"_1\")[0].split(\"(\")[0].replace(\" \", \"\\n\")]\n",
    "\n",
    "        x_ticks = [v for v in x]\n",
    "        for j in range(len(x_tick_labels) - 1, 0, -1):\n",
    "            if x_tick_labels[j] == x_tick_labels[j - 1]:\n",
    "                del x_tick_labels[j]\n",
    "                x_ticks[j - 1] = (x_ticks[j - 1] + x_ticks[j]) / 2\n",
    "                del x_ticks[j]\n",
    "\n",
    "        print(len(x_tick_labels))\n",
    "        #x_ticks = (x[:-1:2] + x[2 - 1::2]) / 2# - single_bar_offset\n",
    "        ax.set_xticks(x_ticks, x_tick_labels, rotation=45 if rotate_ticks else 0)\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "\n",
    "    if not yticks:\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel, labelpad=-10)\n",
    "    ax.set_ylim((0, y_max))\n",
    "    \n",
    "    x_max = max(x)\n",
    "\n",
    "    if legend:\n",
    "        hdls = [ax.scatter([], [], color=colors[k.lower()] if k is not None else \"white\", alpha=0.0 if k is None else 1.0) for k in legend_color_names]\n",
    "        ax.legend(hdls, legend_labels, frameon=False, ncol=legend_cols, handletextpad=0.4, columnspacing=1)\n",
    "\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.set_xlim(-1.0 * bar_width, x_max + 0.75 * bar_width)\n",
    "    ax.spines[\"bottom\"].set_bounds((-0.75 * bar_width, x_max + 0.75 * bar_width))\n",
    "\n",
    "    if grid:\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(linestyle=\"dashed\", axis=\"y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "confidence_means = {}\n",
    "relevant_experiments = [k for k in experiments if \"hard\" not in k.lower()]\n",
    "def tmp_f(df):\n",
    "    df[\"count\"] = len(df)\n",
    "    return df.mean(numeric_only=True)\n",
    "for re in relevant_experiments:\n",
    "    if use_flawed_data:\n",
    "        tmp = dfs_results_main_mp_or_rejected[re].groupby([\"confidence\"], group_keys=False).apply(tmp_f)\n",
    "    else:\n",
    "        tmp = dfs_results_main_passed[re].groupby([\"confidence\"], group_keys=False).apply(tmp_f)\n",
    "    if len(tmp) > 0:\n",
    "        tmp[\"count\"] /= tmp[\"count\"].sum()\n",
    "    for c in (1, 2, 3):\n",
    "        if len(tmp) == 0:\n",
    "            confidence_means[f\"{re}_{c}\"] = tmp.copy().mean()\n",
    "            confidence_means[f\"{re}_{c}\"][\"count\"] = np.nan\n",
    "        else:\n",
    "            confidence_means[f\"{re}_{c}\"] = tmp.loc[c]\n",
    "plot_stacked_confidence(confidence_means, list(confidence_means.keys()), bar_width=0.6, legend_cols=4, y_max=1.15)\n",
    "plt.xlabel(\"Model (Ordered by Scale)\")\n",
    "plt.savefig(os.path.join(results_dir, f\"model_comparison_confidence_distribution.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of performance with model properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_experiments = [k for k in experiments if \"hard\" not in k.lower()]\n",
    "\n",
    "accuracies = {\n",
    "    \"ResNet\": 0.7613,\n",
    "    \"Robust ResNet\": 0.6238,\n",
    "    \"GoogLeNet\": 0.6915,\n",
    "    \"Clip ResNet\": 0.7430,  # The first number is linear probing, 0-shot equals: 0.5983\n",
    "    \"WideResNet\": 0.8160,  # Using 232 instead of 256 resize before central crop\n",
    "    \"DenseNet\": 0.7689,\n",
    "    \"ConvNeXT\": 0.838,\n",
    "    \"Clip ViT\": 0.666,\n",
    "    \"ViT\": 0.74904\n",
    "}\n",
    "\n",
    "mean_ranks = {\n",
    "    \"ResNet\": 7 + 1/3,\n",
    "    \"Robust ResNet\": 5 + 1/3,\n",
    "    \"GoogLeNet\": 9,\n",
    "    \"Clip ResNet\": 3,  # The first number is linear probing, 0-shot equals: 0.5983\n",
    "    \"WideResNet\": 5 + 2/3,  # Using 232 instead of 256 resize before central crop\n",
    "    \"DenseNet\": 4 + 2/3,\n",
    "    \"ConvNeXT\": 4,\n",
    "    \"Clip ViT\": 1,\n",
    "    \"ViT\": 5\n",
    "}\n",
    "\n",
    "data = []\n",
    "for k in relevant_experiments:\n",
    "    if len(dfs_results_main_passed[k]) == 0:\n",
    "        continue\n",
    "    model = k.replace(\" (natural)\", \"\").replace(\" (synthetic)\", \"\")\n",
    "    mode = \"natural\" if \"natural\" in k else \"synthetic\"\n",
    "    np.array([\n",
    "    (max(0, -confidences[k].confidence_interval[0] + means[k][\"correct\"]),\n",
    "     max(0, confidences[k].confidence_interval[1] - means[k][\"correct\"]))\n",
    "     for k in experiments]).T\n",
    "    data.append({\n",
    "        \"model\": model,\n",
    "        \"mean_proportion_correct\": means[k][\"correct\"],\n",
    "        \"confidence_interval_proportion_correct\": (\n",
    "        max(0, -confidences[k].confidence_interval[0] + means[k][\"correct\"]),\n",
    "        max(0, confidences[k].confidence_interval[1] - means[k][\"correct\"])\n",
    "        ),\n",
    "        \"accuracy\": accuracies[model],\n",
    "        \"mean_rank\": mean_ranks[model],\n",
    "        \"mode\": mode\n",
    "        })\n",
    "data = pd.DataFrame(data).set_index(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(4, 3)\n",
    "for mode in (\"natural\", \"synthetic\"):\n",
    "    ax.errorbar(data[data[\"mode\"] == mode][\"accuracy\"], data[data[\"mode\"] == mode][\"mean_proportion_correct\"],\n",
    "                yerr=np.stack(data[data[\"mode\"] == mode][\"confidence_interval_proportion_correct\"], 1), linestyle=\"None\", capsize=5, color=colors[mode])\n",
    "    ax.scatter(data[data[\"mode\"] == mode][\"accuracy\"], data[data[\"mode\"] == mode][\"mean_proportion_correct\"], color=colors[mode])\n",
    "\n",
    "ax.set_ylim((0.55, 0.95))\n",
    "ax.set_xlim(0.59, 0.85)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_bounds((0.6, 0.85))\n",
    "ax.spines[\"left\"].set_bounds((0.55, 0.95))\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(linestyle=\"dashed\", axis=\"y\")\n",
    "\n",
    "ax.set_ylabel(\"Proportion Correct\")\n",
    "ax.set_xlabel(\"ImageNet Validation Top-1 Accuracy\")\n",
    "\n",
    "ax.legend((plt.scatter([],[], c=colors[\"natural\"]), plt.scatter([],[], c=colors[\"synthetic\"])), (\"Natural\", \"Synthetic\"), frameon=False, ncol=2, bbox_to_anchor=(0.5, 0.935), loc=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, \"model_accuracy_vs_performance.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With human-likeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(4, 3)\n",
    "for mode in (\"natural\", \"synthetic\"):\n",
    "    ax.errorbar(data[data[\"mode\"] == mode][\"mean_rank\"], data[data[\"mode\"] == mode][\"mean_proportion_correct\"],\n",
    "                yerr=np.stack(data[data[\"mode\"] == mode][\"confidence_interval_proportion_correct\"], 1), linestyle=\"None\", capsize=5, color=colors[mode])\n",
    "    ax.scatter(data[data[\"mode\"] == mode][\"mean_rank\"], data[data[\"mode\"] == mode][\"mean_proportion_correct\"], color=colors[mode])\n",
    "\n",
    "ax.set_ylim((0.55, 0.95))\n",
    "ax.set_xlim(0.5, 9.5)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_bounds((1, 9))\n",
    "ax.spines[\"left\"].set_bounds((0.55, 0.95))\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(linestyle=\"dashed\", axis=\"y\")\n",
    "\n",
    "ax.set_ylabel(\"Proportion Correct\")\n",
    "ax.set_xlabel(\"Inverse Human-Likeness (Mean Model Rank)\")\n",
    "\n",
    "ax.set_xticks([1, 3, 5, 7, 9])\n",
    "\n",
    "ax.legend((plt.scatter([],[], c=colors[\"natural\"]), plt.scatter([],[], c=colors[\"synthetic\"])), (\"Natural\", \"Synthetic\"), frameon=False, ncol=2, bbox_to_anchor=(0.5, 0.935), loc=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, \"model_human_likeness_vs_performance.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_experiments = [k for k in experiments if \"hard\" not in k.lower()]\n",
    "\n",
    "for mode in (\"natural\", \"synthetic\"):\n",
    "    if use_flawed_data:\n",
    "        relevant_dfs_results_main_passed = [dfs_results_main_mp_or_rejected[k] for k in relevant_experiments if mode in k]\n",
    "    else:\n",
    "        relevant_dfs_results_main_passed = [dfs_results_main_passed[k] for k in relevant_experiments if mode in k]\n",
    "    relevant_dfs_results_main_passed = [it for it in relevant_dfs_results_main_passed if len(it) > 0]\n",
    "    print(\"Mode:\", mode, \"Number of trials:\", sum(len(it) for it in relevant_dfs_results_main_passed))\n",
    "    utils_analysis.run_kruskal_wallis(relevant_dfs_results_main_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_mean(df):\n",
    "    \"\"\"Compute a mean over a (grouped) dataset while gracefully treating non-numeric columns.\"\"\"\n",
    "    new_df = {}\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            try:\n",
    "                unique_values = df[col].unique()\n",
    "\n",
    "                if len(unique_values) == 1:\n",
    "                    value = unique_values[0]\n",
    "                else:\n",
    "                    value = np.nan\n",
    "            except:\n",
    "                value = np.nan\n",
    "        else:\n",
    "            value = df[col].mean()\n",
    "        new_df[col] = value\n",
    "        \n",
    "    return pd.Series(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(dfs):\n",
    "    tmp_dfs = []\n",
    "    for k in dfs:\n",
    "        if len(dfs[k]) == 0:\n",
    "            continue\n",
    "\n",
    "        tmp_df = dfs[k].groupby([\"layer\", \"channel\"]).apply(lambda x: safe_mean(x))\n",
    "        tmp_df[\"model_condition\"] = k\n",
    "        tmp_df[\"unit\"] = tmp_df[\"layer\"] + \":\" + tmp_df[\"channel\"]\n",
    "        tmp_dfs.append(tmp_df)\n",
    "    return pd.concat(tmp_dfs)\n",
    "\n",
    "df_results_main_passed_units_merged = merge_dfs({k: dfs_results_main_passed[k] for k in experiments if \"hard\" not in k.lower()})\n",
    "df_results_main_mp_or_rejected_units_merged = merge_dfs({k: dfs_results_main_mp_or_rejected[k] for k in experiments if \"hard\" not in k.lower()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = {\n",
    "    \"ResNet\": \"resnet50\",\n",
    "    \"Rob. ResNet\": \"resnet50-l2\",\n",
    "    \"GoogLeNet\": \"googlenet\",\n",
    "    \"Clip ResNet\": \"clip-resnet50\",\n",
    "    \"WideResNet\": \"wide_resnet50\",\n",
    "    \"DenseNet\": \"densenet_201\",\n",
    "    \"ConvNeXT\": \"convnext_b\",\n",
    "    \"Clip ViT\": \"clip-vit_b32\",\n",
    "    \"ViT\": \"in1k-vit_b32\",\n",
    "}\n",
    "\n",
    "# Format: diagonal, non-significant, p<0.001, p<0.01, p<0.05\n",
    "cmap = ['1', '#fb6a4a',  '#08306b',  '#4292c6', '#c6dbef']\n",
    "\n",
    "for mode in (\"natural\", \"optimized\"):\n",
    "    print(\"Mode:\", mode)\n",
    "    pc = scikit_posthocs.posthoc_conover(\n",
    "        df_results_main_mp_or_rejected_units_merged[df_results_main_mp_or_rejected_units_merged[\"mode\"] == mode] if use_flawed_data else df_results_main_passed_units_merged[df_results_main_passed_units_merged[\"mode\"] == mode],\n",
    "        val_col=\"correct\",\n",
    "        group_col=\"model\",\n",
    "        p_adjust=\"holm\")\n",
    "    \n",
    "    pc = pc.rename(index={model_names[k]: k for k in model_names})\n",
    "    pc = pc.rename(columns={model_names[k]: k for k in model_names})\n",
    "\n",
    "    display(pc)\n",
    "    hax,_ = scikit_posthocs.sign_plot(pc, cmap=cmap, linewidth=1, square=True, cbar_ax_bbox=[-0.3, -0.25, 0.035, 0.25])\n",
    "    #hax.set_xticklabels(hax.get_yticklabels(), rotation = 0)\n",
    "    #hax.set_yticklabels(hax.get_xticklabels(), rotation = 0)\n",
    "    hax.figure.set_size_inches(2, 2)\n",
    "    plt.savefig(os.path.join(results_dir, f\"model_comparison_significance_{mode}.pdf\"), bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusion Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keys = (\"catch_trials_result\", \"row_variability_result\",\n",
    "        \"total_response_time_result\", \"instruction_time_result\",\n",
    "        \"demo_trials_result\")\n",
    "fig, axs = plt.subplots(1, len(keys), figsize=(1.75*len(keys), 2.4), sharey=True)\n",
    "axs = axs.flatten()\n",
    "df_checks = pd.concat(dfs_checks, axis=0).reset_index(drop=True)\n",
    "for k, ax in zip(keys, axs):\n",
    "    df_checks[k].value_counts().plot(kind=\"bar\", ax=ax)\n",
    "\n",
    "    value_name = k.replace(\"_extracted\", \"\")\n",
    "    value_name = value_name.replace(\"_result\", \"\").replace(\"demo\", \"practice\")\n",
    "    value_name = \" \".join([w.capitalize() for w in value_name.split(\"_\")])\n",
    "\n",
    "    ax.set_title(value_name)\n",
    "\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "axs[2].set_xlabel(\"Passed Exclusion Criteria\")\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "\n",
    "\n",
    "del df_checks\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"exclusion_criteria_decision_distribution.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keys = ('instruction_time_details_extracted',\n",
    "       'total_response_time_details_extracted',\n",
    "       'row_variability_details_details_upper_extracted',\n",
    "       'row_variability_details_details_lower_extracted',\n",
    "       'catch_trials_details_ratio_extracted',\n",
    "       'demo_trials_details_extracted')\n",
    "thresholds = (\n",
    "    (15, None),\n",
    "    (135, 2500),\n",
    "    (5, 40),\n",
    "    (5, 40),\n",
    "    (0.8, None),\n",
    "    (None, 3)\n",
    ")\n",
    "df_checks = pd.concat(dfs_checks, axis=0).reset_index(drop=True)\n",
    "fig, axs = plt.subplots(int(np.ceil(len(keys) / 3)), 3, figsize=(8, 5))\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")\n",
    "for k, ax, ths in zip(keys, axs, thresholds):\n",
    "    ax.axis(\"on\")\n",
    "    ax.hist(df_checks[k], bins=20)\n",
    "    value_name = k.replace(\"_extracted\", \"\")\n",
    "    value_name = value_name.replace(\"_details\", \"\").replace(\"demo\", \"practice\")\n",
    "    value_name = \" \".join([w.capitalize() for w in value_name.split(\"_\")])\n",
    "    ax.set_xlabel(value_name)\n",
    "\n",
    "    if ths[0] is not None:\n",
    "        ax.vlines(ths[0], 0, ax.get_ylim()[1], color=\"red\", linestyle=\"dashed\")\n",
    "    if ths[1] is not None:\n",
    "        ax.vlines(ths[1], 0, ax.get_ylim()[1], color=\"black\", linestyle=\"dashed\")\n",
    "\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "axs[0].legend((axs[0].plot([], [], c=\"red\", ls=\"dashed\")[0], axs[0].plot([], [], c=\"black\", ls=\"dashed\")[0]), (\"Min. Required\", \"Max. Allowed\"), frameon=False, ncol=1)\n",
    "\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "axs[3].set_ylabel(\"Count\")\n",
    "del df_checks\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"exclusion_criteria_value_distribution.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dfs_checks:\n",
    "    if len(dfs_checks[k]) == 0:\n",
    "        continue\n",
    "    print(k, dfs_checks[k][\"catch_trials_details_correctly_answered_extracted\"].map(lambda x: sum(x) / len(x)).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in experiments:\n",
    "    assert k in dfs_results, f\"Missing results for {k}\"\n",
    "\n",
    "joined_df = pd.concat(dfs_results.values(), axis=0).reset_index(drop=True)\n",
    "\n",
    "print(\"#Responses:\", len(joined_df))\n",
    "print(\"#Unique Participants:\", len(joined_df[\"worker_id\"].unique()))\n",
    "\n",
    "unique_participant_id_map = {k: f\"P{i + 1:04}\" for i, k in enumerate(joined_df[\"participant_id\"].unique())}\n",
    "\n",
    "joined_df[\"min_query\"] = \"anonymized\"\n",
    "joined_df[\"max_query\"] = \"anonymized\"\n",
    "joined_df[\"min_references\"] = \"anonymized\"\n",
    "joined_df[\"max_references\"] = \"anonymized\"\n",
    "joined_df[\"participant_id\"] = joined_df[\"participant_id\"].map(lambda pid: unique_participant_id_map[pid])\n",
    "del joined_df[\"worker_id\"]\n",
    "del joined_df[\"result_file_name\"]\n",
    "del joined_df[\"result_creation_time\"]\n",
    "del joined_df[\"query_path\"]\n",
    "\n",
    "# Transform int columns containing NaNs to Int64Dtype to properly represent\n",
    "# NaNs after serialization\n",
    "for c in [\"batch\", \"channel\"]:\n",
    "    vs = joined_df[c].copy()\n",
    "    vs = vs.astype(np.float32)\n",
    "    vs[vs == np.nan] = pd.NA\n",
    "    joined_df[c] = vs.astype(pd.Int64Dtype())\n",
    "\n",
    "joined_df_passed_and_no_mp = joined_df[~joined_df[\"multi_participation\"] & joined_df[\"passed_checks\"]].reset_index(drop=True)\n",
    "joined_df_rejected_or_mp = joined_df[joined_df[\"multi_participation\"] | ~joined_df[\"passed_checks\"]].reset_index(drop=True)\n",
    "\n",
    "joined_df.to_csv(\"results/responses_all.csv\", index=False)\n",
    "joined_df.to_pickle(\"results/responses_all.pd.pkl\")\n",
    "\n",
    "joined_df_passed_and_no_mp.to_csv(\"results/responses_main.csv\", index=False)\n",
    "joined_df_passed_and_no_mp.to_pickle(\"results/responses_main.pd.pkl\")\n",
    "\n",
    "joined_df_rejected_or_mp.to_csv(\"results/responses_lower_quality.csv\", index=False)\n",
    "joined_df_rejected_or_mp.to_pickle(\"results/responses_lower_quality.pd.pkl\")\n",
    "\n",
    "print(\"Responses:\", len(joined_df))\n",
    "print(\"Responses (Passed Checks and No Multi-Participation):\", len(joined_df_passed_and_no_mp))\n",
    "print(\"Responses (Rejected or Multi-Participation):\", len(joined_df_rejected_or_mp))\n",
    "\n",
    "assert len(joined_df) == len(joined_df_passed_and_no_mp) + len(joined_df_rejected_or_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
